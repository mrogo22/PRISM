{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysubgroup as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import pickle\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.dynamic_factor import DynamicFactor\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_usa = pd.read_pickle('AggregatedDataUSA.pkl')\n",
    "aggregated_data_eu = pd.read_pickle('AggregatedDataEU.pkl')\n",
    "aggregated_data_apa = pd.read_pickle('AggregatedDataAPA.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups_eu = pd.read_csv('ResultsSgDisEU.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups_apa = pd.read_csv('ResultsSgDisAPA.csv')\n",
    "subgroups_usa = pd.read_csv('ResultsSgDisUSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups_usa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the data from a long to a wide format for ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_long_df = aggregated_data_eu.copy()\n",
    "usa_long_df = aggregated_data_usa.copy()\n",
    "apa_long_df = aggregated_data_apa.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_long_df = (\n",
    "    eu_long_df\n",
    "    .assign(IndividualPE=eu_long_df['8ClippedResampledInflationPE'])\n",
    "    .explode('IndividualPE')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "eu_long_df['TimePeriod'] = eu_long_df.groupby('ItemNumber').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_long_df = (\n",
    "    usa_long_df\n",
    "    .assign(IndividualPE=usa_long_df['8ClippedResampledInflationPE'])\n",
    "    .explode('IndividualPE')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "usa_long_df['TimePeriod'] = usa_long_df.groupby('ItemNumber').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apa_long_df = (\n",
    "    apa_long_df\n",
    "    .assign(IndividualPE=apa_long_df['8ClippedResampledInflationPE'])\n",
    "    .explode('IndividualPE')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "apa_long_df['TimePeriod'] = apa_long_df.groupby('ItemNumber').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_wide_df = eu_long_df.pivot(\n",
    "            index='TimePeriod',\n",
    "            columns='ItemNumber',\n",
    "            values='IndividualPE'\n",
    "        )\n",
    "eu_wide_df.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_wide_df = usa_long_df.pivot(\n",
    "            index='TimePeriod',\n",
    "            columns='ItemNumber',\n",
    "            values='IndividualPE'\n",
    "        )\n",
    "usa_wide_df.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apa_wide_df = apa_long_df.pivot(\n",
    "            index='TimePeriod',\n",
    "            columns='ItemNumber',\n",
    "            values='IndividualPE'\n",
    "        )\n",
    "apa_wide_df.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data in Subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dict back into memory\n",
    "with open('usa_subgroup_dfs_dict_indexed.pkl', 'rb') as f:\n",
    "    usa_subgroup_dfs_dict_indexed = pickle.load(f)\n",
    "\n",
    "# Verify you got it back\n",
    "print(type(usa_subgroup_dfs_dict_indexed))  # should be <class 'dict'>\n",
    "for sg_index, df in usa_subgroup_dfs_dict_indexed.items():\n",
    "    print(f\"Subgroup {sg_index!r} → {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dict back into memory\n",
    "with open('eu_subgroup_dfs_dict_indexed.pkl', 'rb') as f:\n",
    "    eu_subgroup_dfs_dict_indexed = pickle.load(f)\n",
    "\n",
    "# Verify you got it back\n",
    "print(type(eu_subgroup_dfs_dict_indexed))  # should be <class 'dict'>\n",
    "for sg_index, df in eu_subgroup_dfs_dict_indexed.items():\n",
    "    print(f\"Subgroup {sg_index!r} → {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dict back into memory\n",
    "with open('apa_subgroup_dfs_dict_indexed.pkl', 'rb') as f:\n",
    "    apa_subgroup_dfs_dict_indexed = pickle.load(f)\n",
    "\n",
    "# Verify you got it back\n",
    "print(type(apa_subgroup_dfs_dict_indexed))  # should be <class 'dict'>\n",
    "for sg_index, df in apa_subgroup_dfs_dict_indexed.items():\n",
    "    print(f\"Subgroup {sg_index!r} → {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_subgroup_dfs(dfs_dict, long_name_tpl, wide_name_tpl):\n",
    "    \"\"\"\n",
    "    For each subgroup DataFrame in dfs_dict, creates:\n",
    "      - A long-form DataFrame named long_name_tpl.format(idx=idx)\n",
    "      - A wide-form DataFrame named wide_name_tpl.format(idx=idx)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dfs_dict : dict\n",
    "        Mapping from subgroup index (int) to DataFrame containing\n",
    "        ['ItemNumber', '8ClippedResampledInflationPE'].\n",
    "    long_name_tpl : str\n",
    "        Template for naming long-form globals\n",
    "    wide_name_tpl : str\n",
    "        Template for naming wide-form globals\n",
    "    \"\"\"\n",
    "    cols = ['ItemNumber', '8ClippedResampledInflationPE']\n",
    "    \n",
    "    for idx, df in dfs_dict.items():\n",
    "        # Generate long-form DataFrame\n",
    "        long_name = long_name_tpl.format(idx=idx)\n",
    "        df_long = (\n",
    "            df[cols]\n",
    "            .assign(IndividualPE=df['8ClippedResampledInflationPE'])\n",
    "            .explode('IndividualPE')\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        df_long['TimePeriod'] = df_long.groupby('ItemNumber').cumcount() + 1\n",
    "        \n",
    "        # Store in globals\n",
    "        globals()[long_name] = df_long\n",
    "        \n",
    "        # Generate wide-form DataFrame\n",
    "        wide_name = wide_name_tpl.format(idx=idx)\n",
    "        df_wide = df_long.pivot(\n",
    "            index='TimePeriod',\n",
    "            columns='ItemNumber',\n",
    "            values='IndividualPE'\n",
    "        )\n",
    "        df_wide.columns.name = None\n",
    "        \n",
    "        # Store in globals\n",
    "        globals()[wide_name] = df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_subgroup_dfs(\n",
    "     eu_subgroup_dfs_dict_indexed,\n",
    "     long_name_tpl=\"eu_sg_{idx}_long\",\n",
    "     wide_name_tpl=\"eu_sg_{idx}_wide\"\n",
    " )\n",
    "\n",
    "transform_subgroup_dfs(\n",
    "     apa_subgroup_dfs_dict_indexed,\n",
    "     long_name_tpl=\"apa_sg_{idx}_long\",\n",
    "     wide_name_tpl=\"apa_sg_{idx}_wide\"\n",
    " )\n",
    "\n",
    "transform_subgroup_dfs(\n",
    "     usa_subgroup_dfs_dict_indexed,\n",
    "     long_name_tpl=\"usa_sg_{idx}_long\",\n",
    "     wide_name_tpl=\"usa_sg_{idx}_wide\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_mint_forecast(\n",
    "    df_wide: pd.DataFrame,\n",
    "    h: int = 1,\n",
    "    seasonal: bool = False,\n",
    "    m: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform Hierarchical + (OLS‐style MinT) reconciliation on a subgroup.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_wide : DataFrame\n",
    "        Historical elasticity panel of shape (T × N), where\n",
    "        - index = TimePeriod (1…8 or actual dates)\n",
    "        - columns = ItemNumber\n",
    "        - values = IndividualPE\n",
    "    h : int\n",
    "        Forecast horizon (e.g. 1 for one‐step‐ahead).\n",
    "    seasonal : bool\n",
    "        Whether to include seasonal terms in auto_arima.\n",
    "    m : int\n",
    "        Seasonal period (only if seasonal=True).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    reconciled_df : DataFrame\n",
    "        Reconciled bottom‐level forecasts of shape (h × N), with the same\n",
    "        columns as df_wide, and a future index (TimePeriod T+1 … T+h).\n",
    "    \"\"\"\n",
    "    # 1) Build the top‐level (aggregate) series\n",
    "    #    You can sum or average; we’ll use the mean here:\n",
    "    agg_series = df_wide.mean(axis=1)\n",
    "\n",
    "    # 2) Fit ARIMA to the aggregate\n",
    "    print(\"Starting ARIMA - training model on subgroup ...\")\n",
    "    model_agg = auto_arima(\n",
    "        agg_series,\n",
    "        seasonal=seasonal,\n",
    "        m=m,\n",
    "        error_action='ignore',\n",
    "        suppress_warnings=True\n",
    "    )\n",
    "    print(\"Model trained on subgroup - staring general prediction...\")\n",
    "    #    Produce h‐step‐ahead aggregate forecast\n",
    "    f_agg = model_agg.predict(n_periods=h)            # shape (h,)\n",
    "    \n",
    "    # 3) Fit bottom‐level models (one per product)\n",
    "    product_ids = df_wide.columns.tolist()\n",
    "    print(f\"General prediction done! - starting individual product training on {len(product_ids)} products\")\n",
    "    bottom_forecasts = []\n",
    "    count = 0\n",
    "    passed_75 = False\n",
    "    passed_50 = False\n",
    "    passed_25 = False\n",
    "    for pid in product_ids:\n",
    "        count+=1\n",
    "        if count / len(product_ids) >= 0.75 and passed_75 == False:\n",
    "            print(\"75% of products trained!\")\n",
    "            passed_75 = True\n",
    "        if count / len(product_ids) >= 0.5 and passed_50 == False:\n",
    "            print(\"50% of products trained!\")\n",
    "            passed_50 = True\n",
    "        if count / len(product_ids) >= 0.25 and passed_25 == False:\n",
    "            print(\"25% of products trained!\")\n",
    "            passed_25 = True\n",
    "\n",
    "        m_b = auto_arima(\n",
    "            df_wide[pid],\n",
    "            seasonal=seasonal,\n",
    "            m=m,\n",
    "            error_action='ignore',\n",
    "            suppress_warnings=True\n",
    "        )\n",
    "        bottom_forecasts.append(m_b.predict(n_periods=h))\n",
    "    \n",
    "    print(\"Individual product training done! - starting reconciliation...\")\n",
    "\n",
    "    # Stack into array of shape (N_products × h)\n",
    "    f_b = np.vstack(bottom_forecasts)                 # shape (N, h)\n",
    "    n_products = f_b.shape[0]\n",
    "\n",
    "    # 4) OLS‐style MinT reconciliation (identity W)\n",
    "    #    Compute per‐horizon correction so bottoms sum to aggregate\n",
    "    correction = (f_agg - f_b.sum(axis=0)) / n_products  # shape (h,)\n",
    "    # force it into a 1-D NumPy array\n",
    "    correction = np.asarray(correction)\n",
    "    f_b_rec = f_b + correction[np.newaxis, :]           # shape (N, h)\n",
    "\n",
    "    # 5) Wrap into a DataFrame with a future TimePeriod index\n",
    "    last_period = df_wide.index.max()\n",
    "    future_index = range(last_period + 1, last_period + 1 + h)\n",
    "    reconciled_df = pd.DataFrame(\n",
    "        f_b_rec.T,               # transpose to (h, N)\n",
    "        index=future_index,\n",
    "        columns=product_ids\n",
    "    )\n",
    "    print(\"Reconciliation done! Results are ready.\")\n",
    "\n",
    "    return reconciled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dict to hold reconciled forecasts per subgroup\n",
    "usa_hierarchical_forecasts = {}\n",
    "\n",
    "# Loop over your subgroup indices (assuming you still have usa_subgroup_dfs_dict_indexed)\n",
    "for idx in usa_subgroup_dfs_dict_indexed:\n",
    "    # Grab the wide DataFrame from globals\n",
    "\n",
    "    print(f\"Now predicting subgroup {idx}\")\n",
    "    wide_name = f\"usa_sg_{idx}_wide\"\n",
    "    df_wide = globals()[wide_name]\n",
    "    \n",
    "    # Run the Hierarchical + MinT forecast (one‐step‐ahead)\n",
    "    # You can change h=1 to whatever horizon you need\n",
    "    reconciled_df = hierarchical_mint_forecast(df_wide, h=1, seasonal=False)\n",
    "    \n",
    "    # Store it\n",
    "    usa_hierarchical_forecasts[idx] = reconciled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dict to hold reconciled forecasts per subgroup\n",
    "eu_hierarchical_forecasts = {}\n",
    "\n",
    "# Loop over your subgroup indices (assuming you still have eu_subgroup_dfs_dict_indexed)\n",
    "for idx in eu_subgroup_dfs_dict_indexed:\n",
    "    # Grab the wide DataFrame from globals\n",
    "\n",
    "    print(f\"Now predicting subgroup {idx}\")\n",
    "    wide_name = f\"eu_sg_{idx}_wide\"\n",
    "    df_wide = globals()[wide_name]\n",
    "    \n",
    "    # Run the Hierarchical + MinT forecast (one‐step‐ahead)\n",
    "    # You can change h=1 to whatever horizon you need\n",
    "    reconciled_df = hierarchical_mint_forecast(df_wide, h=1, seasonal=False)\n",
    "    \n",
    "    # Store it\n",
    "    eu_hierarchical_forecasts[idx] = reconciled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dict to hold reconciled forecasts per subgroup\n",
    "apa_hierarchical_forecasts = {}\n",
    "\n",
    "# Loop over your subgroup indices (assuming you still have apa_subgroup_dfs_dict_indexed)\n",
    "for idx in apa_subgroup_dfs_dict_indexed:\n",
    "    # Grab the wide DataFrame from globals\n",
    "\n",
    "    print(f\"Now predicting subgroup {idx}\")\n",
    "    wide_name = f\"apa_sg_{idx}_wide\"\n",
    "    df_wide = globals()[wide_name]\n",
    "    \n",
    "    # Run the Hierarchical + MinT forecast (one‐step‐ahead)\n",
    "    # You can change h=1 to whatever horizon you need\n",
    "    reconciled_df = hierarchical_mint_forecast(df_wide, h=1, seasonal=False)\n",
    "    \n",
    "    # Store it\n",
    "    apa_hierarchical_forecasts[idx] = reconciled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define Directional Magnitude Score (DMS)\n",
    "def directional_magnitude_score(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    train: np.ndarray,\n",
    "    w: float = 0.5,\n",
    "    R: float = 25.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Directional Magnitude Score (0–1):\n",
    "      D = 1 if sign(y_pred) == sign(y_true), else 0\n",
    "      M = max(0, 1 - abs(y_pred - y_true) / R)\n",
    "      DMS = w * D + (1 - w) * M\n",
    "\n",
    "    Returns np.nan if train is constant.\n",
    "    \"\"\"\n",
    "    # Exclude constant-series\n",
    "    if np.allclose(train, train[0]):\n",
    "        return np.nan\n",
    "\n",
    "    # Flatten in case of multi-step h > 1\n",
    "    y_true = np.ravel(y_true)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "\n",
    "    scores = []\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        # 1) Direction correctness\n",
    "        D = 1.0 if np.sign(yt) == np.sign(yp) else 0.0\n",
    "        # 2) Magnitude closeness\n",
    "        M = max(0.0, 1.0 - abs(yp - yt) / R)\n",
    "        # 3) Combined score\n",
    "        scores.append(w * D + (1 - w) * M)\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "def evaluate_hierarchical_dms_all(\n",
    "    subgroup_indices,\n",
    "    wide_name_tpl,\n",
    "    folds: list = [5, 6, 7],\n",
    "    h: int = 1,\n",
    "    w: float = 0.5,\n",
    "    R: float = 25.0\n",
    "):\n",
    "    \"\"\"\n",
    "    For each subgroup:\n",
    "      • Computes the mean DMS (Directional Magnitude Score) across all products & folds,\n",
    "        prints it.\n",
    "      • Stores the mean DMS per product for later analysis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subgroup_scores : dict\n",
    "        { idx -> mean DMS over products }\n",
    "    per_product_scores : dict\n",
    "        { idx -> { product_id -> mean DMS for that product } }\n",
    "    \"\"\"\n",
    "    subgroup_scores = {}\n",
    "    per_product_scores = {}\n",
    "\n",
    "    for idx in subgroup_indices:\n",
    "        # grab the wide panel from globals\n",
    "        df_wide = globals()[wide_name_tpl.format(idx=idx)]\n",
    "        \n",
    "        # prepare a place to collect per-fold DMS per product\n",
    "        errors = {pid: [] for pid in df_wide.columns}\n",
    "\n",
    "        # rolling‐origin backtest\n",
    "        for t_end in folds:\n",
    "            train    = df_wide.iloc[:t_end]\n",
    "            test     = df_wide.iloc[t_end : t_end + h]\n",
    "            forecast = hierarchical_mint_forecast(train, h=h)\n",
    "\n",
    "            for pid in df_wide.columns:\n",
    "                y_true       = test[pid].values\n",
    "                y_pred       = forecast[pid].values\n",
    "                train_series = train[pid].values\n",
    "                score = directional_magnitude_score(\n",
    "                    y_true, y_pred, train_series, w=w, R=R\n",
    "                )\n",
    "                errors[pid].append(score)\n",
    "\n",
    "        # average over folds for each product\n",
    "        mean_per_product = {\n",
    "            pid: float(np.nanmean(scores))\n",
    "            for pid, scores in errors.items()\n",
    "        }\n",
    "        per_product_scores[idx] = mean_per_product\n",
    "\n",
    "        # average those product‐means into one subgroup score\n",
    "        subgroup_mean = float(np.nanmean(list(mean_per_product.values())))\n",
    "        subgroup_scores[idx] = subgroup_mean\n",
    "\n",
    "        print(f\"Subgroup {idx}: mean DMS = {subgroup_mean:.3f}\")\n",
    "\n",
    "    return subgroup_scores, per_product_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_indices = list(usa_subgroup_dfs_dict_indexed.keys())\n",
    "usa_subgroup_dms, usa_product_dms = evaluate_hierarchical_dms_all(usa_indices, \"usa_sg_{idx}_wide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apa_indices = list(apa_subgroup_dfs_dict_indexed.keys())\n",
    "apa_subgroup_dms, apa_product_dms = evaluate_hierarchical_dms_all(apa_indices, \"apa_sg_{idx}_wide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_indices = list(eu_subgroup_dfs_dict_indexed.keys())\n",
    "eu_subgroup_dms, eu_product_dms = evaluate_hierarchical_dms_all(eu_indices, \"eu_sg_{idx}_wide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Evaluation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Flatten into a list of records\n",
    "records = []\n",
    "for subgroup_idx, prod_scores in usa_product_dms.items():\n",
    "    for item, score in prod_scores.items():\n",
    "        records.append({\n",
    "            'ItemNumber': item,\n",
    "            'Sg Index': subgroup_idx,\n",
    "            'DMS': score,\n",
    "            'Sg DMS': usa_subgroup_dms[subgroup_idx],\n",
    "        })\n",
    "\n",
    "# 2) Create DataFrame\n",
    "usa_dms_df = pd.DataFrame(records)\n",
    "\n",
    "\n",
    "\n",
    "print(usa_dms_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Flatten into a list of records\n",
    "records = []\n",
    "for subgroup_idx, prod_scores in apa_product_dms.items():\n",
    "    for item, score in prod_scores.items():\n",
    "        records.append({\n",
    "            'ItemNumber': item,\n",
    "            'Sg Index': subgroup_idx,\n",
    "            'DMS': score,\n",
    "            'Sg DMS': apa_subgroup_dms[subgroup_idx],\n",
    "        })\n",
    "\n",
    "# 2) Create DataFrame\n",
    "apa_dms_df = pd.DataFrame(records)\n",
    "\n",
    "\n",
    "\n",
    "print(apa_dms_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Flatten into a list of records\n",
    "records = []\n",
    "for subgroup_idx, prod_scores in eu_product_dms.items():\n",
    "    for item, score in prod_scores.items():\n",
    "        records.append({\n",
    "            'ItemNumber': item,\n",
    "            'Sg Index': subgroup_idx,\n",
    "            'DMS': score,\n",
    "            'Sg DMS': eu_subgroup_dms[subgroup_idx],\n",
    "        })\n",
    "\n",
    "# 2) Create DataFrame\n",
    "eu_dms_df = pd.DataFrame(records)\n",
    "\n",
    "\n",
    "\n",
    "print(eu_dms_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN DMS with Sg DMS\n",
    "usa_dms_df['DMS'] = usa_dms_df['DMS'].fillna(usa_dms_df['Sg DMS'])\n",
    "\n",
    "apa_dms_df['DMS'] = apa_dms_df['DMS'].fillna(apa_dms_df['Sg DMS'])\n",
    "\n",
    "eu_dms_df['DMS'] = eu_dms_df['DMS'].fillna(eu_dms_df['Sg DMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_dms_df['DMS'] = usa_dms_df['DMS'].fillna(usa_dms_df['Sg DMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Predicted Elasticities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build a flat mapping from ItemNumber to its one‐step‐ahead forecast\n",
    "predicted_pe_usa = {}\n",
    "for idx, forecast_df in usa_hierarchical_forecasts.items():\n",
    "    # forecast_df is a (1 × N) DataFrame whose columns are ItemNumbers\n",
    "    # and whose single row is the predicted PE for that subgroup\n",
    "    row = forecast_df.iloc[0]           # a Series: index=ItemNumber, value=Predicted PE\n",
    "    predicted_pe_usa.update(row.to_dict())  # add all item→PE pairs into our dict\n",
    "\n",
    "# 2) Map that into your df_all_products\n",
    "usa_dms_df['Predicted PE'] = usa_dms_df['ItemNumber'].map(predicted_pe_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build a flat mapping from ItemNumber to its one‐step‐ahead forecast\n",
    "predicted_pe_apa = {}\n",
    "for idx, forecast_df in apa_hierarchical_forecasts.items():\n",
    "    # forecast_df is a (1 × N) DataFrame whose columns are ItemNumbers\n",
    "    # and whose single row is the predicted PE for that subgroup\n",
    "    row = forecast_df.iloc[0]           # a Series: index=ItemNumber, value=Predicted PE\n",
    "    predicted_pe_apa.update(row.to_dict())  # add all item→PE pairs into our dict\n",
    "\n",
    "# 2) Map that into your df_all_products\n",
    "apa_dms_df['Predicted PE'] = apa_dms_df['ItemNumber'].map(predicted_pe_apa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build a flat mapping from ItemNumber to its one‐step‐ahead forecast\n",
    "predicted_pe_eu = {}\n",
    "for idx, forecast_df in eu_hierarchical_forecasts.items():\n",
    "    # forecast_df is a (1 × N) DataFrame whose columns are ItemNumbers\n",
    "    # and whose single row is the predicted PE for that subgroup\n",
    "    row = forecast_df.iloc[0]           # a Series: index=ItemNumber, value=Predicted PE\n",
    "    predicted_pe_eu.update(row.to_dict())  # add all item→PE pairs into our dict\n",
    "\n",
    "# 2) Map that into your df_all_products\n",
    "eu_dms_df['Predicted PE'] = eu_dms_df['ItemNumber'].map(predicted_pe_eu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_dms_df.to_pickle('prediction_results_usa.pkl')\n",
    "usa_dms_df.to_csv('prediction_results_usa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apa_dms_df.to_pickle('prediction_results_apa.pkl')\n",
    "apa_dms_df.to_csv('prediction_results_apa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_dms_df.to_pickle('prediction_results_eu.pkl')\n",
    "eu_dms_df.to_csv('prediction_results_eu.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
