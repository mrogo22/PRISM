{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Suppress the SettingWithCopyWarning \n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_inflation = pd.read_csv('df_inflation.csv')\n",
    "df_inflation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in the Excel file\n",
    "df_wide_data = pd.read_excel(\"DataPreparationOutputWeird.xlsx\")\n",
    "df_wide_data = df_wide_data[df_wide_data['GLPSalesUoM'] != 0]\n",
    "# Display the first few rows of the dataframe\n",
    "df_wide_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we check for abnormally high prices. This is an error we had before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = (df_wide_data['PriceSalesUoMEUR'] > 100000).sum()\n",
    "print(\"Number of rows where PriceSalesUoMEUR > 100000:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_wide_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data['ItemNumber'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data['CustomerSoldTo'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data = df_wide_data.drop(columns=['OrderDate_y']).rename(columns={'OrderDate_x': 'OrderDate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldest_date = df_wide_data['OrderDate'].min()\n",
    "latest_date = df_wide_data['OrderDate'].max()\n",
    "\n",
    "print(\"Oldest Date:\", oldest_date)\n",
    "print(\"Latest Date:\", latest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data['Discount'] = df_wide_data['GLPSalesUoM'] - df_wide_data['PriceSalesUoMEUR']\n",
    "df_wide_data['Discount'] = df_wide_data['Discount'].astype(float)\n",
    "df_wide_data.loc[df_wide_data['GLPSalesUoM'] == 0, 'Discount'] = 0\n",
    "df_wide_data.loc[df_wide_data['PriceSalesUoMEUR'] == 0, 'Discount'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data['ManualDiscount'] = np.where(\n",
    "    df_wide_data['PriceSalesUoMEUR'] == 0,\n",
    "    0,\n",
    "    df_wide_data['PCoverPriceSalesUoM'] - df_wide_data['PriceSalesUoMEUR']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data['DaysDifference'] = (df_wide_data['ActualShipDate'] - df_wide_data['OriginalPromisedShipDate']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data['LatestDaysDifference'] = (df_wide_data['ActualShipDate'] - df_wide_data['LatestPromisedShipDate']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take only items with variable GLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_glp(df):\n",
    "    items_w_more_than_1_glp = 0\n",
    "    items_w_1_glp =0\n",
    "    items_with_multiple_glp = []\n",
    "    # Group by ItemNumber and find the number of unique PriceSalesUoMEUR values for each\n",
    "    unique__glp_per_item = df.groupby('ItemNumber')['GLPSalesUoM'].nunique()\n",
    "\n",
    "    # Print the result\n",
    "    print(\"Number of unique GLPSalesUoM values per ItemNumber:\")\n",
    "    for item, unique_price in unique__glp_per_item.items():\n",
    "        if(unique_price)>1:\n",
    "            items_w_more_than_1_glp += 1\n",
    "            items_with_multiple_glp.append(item)\n",
    "        else:\n",
    "            items_w_1_glp += 1\n",
    "        #print(f\"ItemNumber: {item}, Unique GLPSalesUoM: {unique_price}\")\n",
    "    print(f\"There are {items_w_more_than_1_glp} items with more than 1 glp\")\n",
    "    print(f\"There are {items_w_1_glp} items with exactly 1 glp\")\n",
    "\n",
    "    return items_with_multiple_glp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_item_numbers = df_wide_data['ItemNumber'].nunique()\n",
    "print(f'There are {unique_item_numbers} unique ItemNumbers in df_wide_data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_glp_items = check_unique_glp(df_wide_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df_wide_data[df_wide_data['ItemNumber'].isin(var_glp_items)].shape[0]\n",
    "print(\"Number of rows with ItemNumber in var_glp_items:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_var_glp = df_wide_data[df_wide_data['ItemNumber'].isin(var_glp_items)]\n",
    "df_wide_data_var_glp_sorted = df_wide_data_var_glp.sort_values(by=['ItemNumber', 'CustomerSoldTo','OrderDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split df by region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point onwards, we split data into multiple dataframes using region. This is why every operation will be done a number of times. We generate one dataframe for USA, one for APA, one for EU, and one for EU and APA together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_usa = df_wide_data[df_wide_data['GeographicRegion'] == 'USA']\n",
    "df_wide_data_usa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_glp_items_usa = check_unique_glp(df_wide_data_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_var_glp_usa = df_wide_data_usa[df_wide_data_usa['ItemNumber'].isin(var_glp_items_usa)]\n",
    "df_wide_data_var_glp_sorted_usa = df_wide_data_var_glp_usa.sort_values(by=['ItemNumber', 'CustomerSoldTo','OrderDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_eu_apa = df_wide_data[df_wide_data['GeographicRegion'] != 'USA']\n",
    "df_wide_data_eu_apa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_eu = df_wide_data[df_wide_data['GeographicRegion'] == 'EUR']\n",
    "df_wide_data_apa = df_wide_data[df_wide_data['GeographicRegion'] == 'APA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_glp_items_eu_apa = check_unique_glp(df_wide_data_eu_apa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_glp_items_eu = check_unique_glp(df_wide_data_eu)\n",
    "var_glp_items_apa = check_unique_glp(df_wide_data_apa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_var_glp_eu_apa = df_wide_data_eu_apa[df_wide_data_eu_apa['ItemNumber'].isin(var_glp_items_eu_apa)]\n",
    "df_wide_data_var_glp_sorted_eu_apa = df_wide_data_var_glp_eu_apa.sort_values(by=['ItemNumber', 'CustomerSoldTo','OrderDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_var_glp_eu = df_wide_data_eu[df_wide_data_eu['ItemNumber'].isin(var_glp_items_eu)]\n",
    "df_wide_data_var_glp_sorted_eu = df_wide_data_var_glp_eu.sort_values(by=['ItemNumber', 'CustomerSoldTo','OrderDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_var_glp_apa = df_wide_data_apa[df_wide_data_apa['ItemNumber'].isin(var_glp_items_apa)]\n",
    "df_wide_data_var_glp_sorted_apa = df_wide_data_var_glp_apa.sort_values(by=['ItemNumber', 'CustomerSoldTo','OrderDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_var_glp_sorted_eu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save region split order data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point, the data is not yet aggregated per product. It is still long and wide order data. We save the region splits in case future changes are necessary in one of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a pickle file\n",
    "df_wide_data_var_glp_sorted_apa.to_pickle('PreparedOrderDataAPA.pkl')\n",
    "df_wide_data_var_glp_sorted_eu_apa.to_pickle('PreparedOrderDataEUandAPA.pkl')\n",
    "df_wide_data_var_glp_sorted_eu.to_pickle('PreparedOrderDataEU.pkl')\n",
    "df_wide_data_var_glp_sorted_usa.to_pickle('PreparedOrderDataUSA.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_sales_variable_glp_sorted with df_inflation on OrderYear and OrderMonth\n",
    "df_wide_data_var_glp_sorted_inflation = df_wide_data_var_glp_sorted.merge(\n",
    "    df_inflation[['Year', 'Month', 'InflationIndex']],\n",
    "    left_on=['OrderYear', 'OrderMonth'],\n",
    "    right_on=['Year', 'Month'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the redundant 'Year' and 'Month' columns from the merged dataframe\n",
    "df_wide_data_var_glp_sorted_inflation.drop(columns=['Year', 'Month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_sales_variable_glp_sorted with df_inflation on OrderYear and OrderMonth\n",
    "df_wide_data_var_glp_sorted_inflation_eu_apa = df_wide_data_var_glp_sorted_eu_apa.merge(\n",
    "    df_inflation[['Year', 'Month', 'InflationIndex']],\n",
    "    left_on=['OrderYear', 'OrderMonth'],\n",
    "    right_on=['Year', 'Month'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the redundant 'Year' and 'Month' columns from the merged dataframe\n",
    "df_wide_data_var_glp_sorted_inflation_eu_apa.drop(columns=['Year', 'Month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_sales_variable_glp_sorted with df_inflation on OrderYear and OrderMonth\n",
    "df_wide_data_var_glp_sorted_inflation_usa = df_wide_data_var_glp_sorted_usa.merge(\n",
    "    df_inflation[['Year', 'Month', 'InflationIndex']],\n",
    "    left_on=['OrderYear', 'OrderMonth'],\n",
    "    right_on=['Year', 'Month'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the redundant 'Year' and 'Month' columns from the merged dataframe\n",
    "df_wide_data_var_glp_sorted_inflation_usa.drop(columns=['Year', 'Month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_sales_variable_glp_sorted with df_inflation on OrderYear and OrderMonth\n",
    "df_wide_data_var_glp_sorted_inflation_eu = df_wide_data_var_glp_sorted_eu.merge(\n",
    "    df_inflation[['Year', 'Month', 'InflationIndex']],\n",
    "    left_on=['OrderYear', 'OrderMonth'],\n",
    "    right_on=['Year', 'Month'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the redundant 'Year' and 'Month' columns from the merged dataframe\n",
    "df_wide_data_var_glp_sorted_inflation_eu.drop(columns=['Year', 'Month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_sales_variable_glp_sorted with df_inflation on OrderYear and OrderMonth\n",
    "df_wide_data_var_glp_sorted_inflation_apa = df_wide_data_var_glp_sorted_apa.merge(\n",
    "    df_inflation[['Year', 'Month', 'InflationIndex']],\n",
    "    left_on=['OrderYear', 'OrderMonth'],\n",
    "    right_on=['Year', 'Month'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the redundant 'Year' and 'Month' columns from the merged dataframe\n",
    "df_wide_data_var_glp_sorted_inflation_apa.drop(columns=['Year', 'Month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_var_glp_sorted_inflation_usa.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate per item-glp\n",
    "The following function is the first step in computing the price elasticity values. For each item, for each GLP that item had, we compute the time that GLP was active, the average inflation index, and the quantity sold at that GLP. We also compute some features, but the numerical ones will be further averaged to product level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_per_item_glp(group_item_glp):\n",
    "    group_item_glp = group_item_glp.sort_values(by='OrderDate')\n",
    "    aggredated_data = [] \n",
    "    start_time = pd.to_datetime(group_item_glp['OrderDate'].iloc[0])\n",
    "    start_year = start_time.year\n",
    "    end_time = pd.to_datetime(group_item_glp['OrderDate'].iloc[-1])\n",
    "    end_year = end_time.year\n",
    "    #effective_from = pd.to_datetime(group_item_glp['EffectiveFrom'].iloc[0])\n",
    "    #effective_to = pd.to_datetime(group_item_glp['EffectiveTo'].iloc[-1])\n",
    "    oem = group_item_glp['OEMName'].iloc[0]\n",
    "    #time_effective = (effective_to - effective_from).days\n",
    "    #if effective_to > pd.Timestamp('2025-04-01'):\n",
    "        #today = pd.Timestamp('2025-04-01')\n",
    "        #time_effective = (today - effective_from).days\n",
    "    time_active = (end_time - start_time).days\n",
    "    avg_inflation_index = group_item_glp['InflationIndex'].mean()\n",
    "    total_quantity = group_item_glp['OrderQuantity'].sum()\n",
    "    group_glp = group_item_glp['GLPSalesUoM'].iloc[0]\n",
    "    group_item = group_item_glp['ItemNumber'].iloc[0]\n",
    "    avg_sales_price = group_item_glp['PriceSalesUoMEUR'][group_item_glp['PriceSalesUoMEUR'] != 0].mean()\n",
    "    avg_cost_price = group_item_glp['CostSalesUoMEUR'].mean()\n",
    "    manual_discount = group_item_glp['ManualDiscount'].mean()\n",
    "    # consolidation_customer = group_item_glp['ConsolidationCustomer'].mean()\n",
    "    supplier_leadtime = group_item_glp['SupplierLeadtime'].mean()\n",
    "    price_category = group_item_glp['PriceCategory'].iloc[0]\n",
    "    percent_eu = (group_item_glp['GeographicRegion'] == 'EUR').mean() * 100\n",
    "    percent_usa = (group_item_glp['GeographicRegion'] == 'USA').mean() * 100\n",
    "    percent_apa = (group_item_glp['GeographicRegion'] == 'APA').mean() * 100\n",
    "    mean_days_diff = group_item_glp['DaysDifference'].mean()\n",
    "    mean_latest_days_diff = group_item_glp['LatestDaysDifference'].mean()\n",
    "    VIEngineered = group_item_glp['VIEngineered'].iloc[0]\n",
    "    SparePartsCategory = group_item_glp['SparePartsCategory'].iloc[0]\n",
    "    TechnicalClassification = group_item_glp['TechnicalClassification'].iloc[0]\n",
    "    split_columns = group_item_glp['TechnicalClassification'].str.split('-', expand=True)\n",
    "    split_columns = split_columns.reindex(columns=range(5))\n",
    "    split_columns.columns = ['level1', 'level2', 'level3', 'level4', 'level5']\n",
    "    \n",
    "\n",
    "    if time_active == 0:\n",
    "        time_active = 1\n",
    "    #if time_effective == 0:\n",
    "        #time_effective = 1 \n",
    "        \n",
    "    aggredated_data.append({ \n",
    "        'ItemNumber': group_item,\n",
    "        'GLPSalesUoM': group_glp,\n",
    "        'StartTime': start_time,\n",
    "        'EndTime': end_time,\n",
    "        'StartYear': start_year,\n",
    "        'EndYear': end_year,\n",
    "        'TimeActive': time_active,\n",
    "        #'EffectiveFrom': effective_from,\n",
    "        #'EffectiveTo': effective_to,\n",
    "        #'TimeEffective': time_effective,\n",
    "        'TotalQuantity': total_quantity,\n",
    "        'AvgInflationIndex': avg_inflation_index,\n",
    "        'AvgSalesPrice': avg_sales_price,\n",
    "        'AvgCostPrice': avg_cost_price,\n",
    "        'ManualDiscount': manual_discount,\n",
    "        # 'ConsolidationCustomer': consolidation_customer,\n",
    "        'SupplierLeadtime': supplier_leadtime,\n",
    "        'PriceCategory': price_category,\n",
    "        'PercentEU': percent_eu,\n",
    "        'PercentUSA': percent_usa,\n",
    "        'PercentAPA': percent_apa,\n",
    "        'OEM': oem,\n",
    "        'MeanDaysDifference': mean_days_diff,\n",
    "        'MeanLatestDaysDifference': mean_latest_days_diff,\n",
    "        'VIEngineered': VIEngineered,\n",
    "        'SparePartsCategory': SparePartsCategory,\n",
    "        'TechnicalClassification': TechnicalClassification,\n",
    "        'Level1': split_columns['level1'].iloc[0],\n",
    "        'Level2': split_columns['level2'].iloc[0],\n",
    "        'Level3': split_columns['level3'].iloc[0],\n",
    "        'Level4': split_columns['level4'].iloc[0],\n",
    "        'Level5': split_columns['level5'].iloc[0]\n",
    "\n",
    "\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(aggredated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggreagated_item_glp = df_wide_data_var_glp_sorted_inflation.sort_values(by=['OrderDate']).groupby(['ItemNumber', 'GLPSalesUoM']).apply(aggregate_per_item_glp)\n",
    "ungrouped_aggregated_item_glp= aggreagated_item_glp.reset_index(drop=True)\n",
    "item_grouped_aggregated = ungrouped_aggregated_item_glp.groupby('ItemNumber')\n",
    "item_grouped_aggregated_sorted = item_grouped_aggregated.apply(lambda x: x.sort_values(by='StartTime')).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_grouped_aggregated_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggreagated_item_glp_usa = df_wide_data_var_glp_sorted_inflation_usa.groupby(['ItemNumber', 'GLPSalesUoM']).apply(aggregate_per_item_glp)\n",
    "ungrouped_aggregated_item_glp_usa= aggreagated_item_glp_usa.reset_index(drop=True)\n",
    "item_grouped_aggregated_usa = ungrouped_aggregated_item_glp_usa.groupby('ItemNumber')\n",
    "item_grouped_aggregated_sorted_usa = item_grouped_aggregated_usa.apply(lambda x: x.sort_values(by='StartTime')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggreagated_item_glp_eu_apa = df_wide_data_var_glp_sorted_inflation_eu_apa.sort_values(by=['OrderDate']).groupby(['ItemNumber', 'GLPSalesUoM']).apply(aggregate_per_item_glp)\n",
    "ungrouped_aggregated_item_glp_eu_apa= aggreagated_item_glp_eu_apa.reset_index(drop=True)\n",
    "item_grouped_aggregated_eu_apa = ungrouped_aggregated_item_glp_eu_apa.groupby('ItemNumber')\n",
    "item_grouped_aggregated_sorted_eu_apa = item_grouped_aggregated_eu_apa.apply(lambda x: x.sort_values(by='StartTime')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggreagated_item_glp_eu = df_wide_data_var_glp_sorted_inflation_eu.sort_values(by=['OrderDate']).groupby(['ItemNumber', 'GLPSalesUoM']).apply(aggregate_per_item_glp)\n",
    "ungrouped_aggregated_item_glp_eu= aggreagated_item_glp_eu.reset_index(drop=True)\n",
    "item_grouped_aggregated_eu = ungrouped_aggregated_item_glp_eu.groupby('ItemNumber')\n",
    "item_grouped_aggregated_sorted_eu = item_grouped_aggregated_eu.apply(lambda x: x.sort_values(by='StartTime')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggreagated_item_glp_apa = df_wide_data_var_glp_sorted_inflation_apa.sort_values(by=['OrderDate']).groupby(['ItemNumber', 'GLPSalesUoM']).apply(aggregate_per_item_glp)\n",
    "ungrouped_aggregated_item_glp_apa= aggreagated_item_glp_apa.reset_index(drop=True)\n",
    "item_grouped_aggregated_apa = ungrouped_aggregated_item_glp_apa.groupby('ItemNumber')\n",
    "item_grouped_aggregated_sorted_apa = item_grouped_aggregated_apa.apply(lambda x: x.sort_values(by='StartTime')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_grouped_aggregated_eu.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate price elasticity\n",
    "The following function computes the array of price elasticities per product, at every point where the GLP changes. This results in a dataframe with a lot of redundancy, with one row per product-elasticity. It will be condensed further into one row per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pe(group_item_aggr):\n",
    "    pe_data = []\n",
    "    group_item = group_item_aggr['ItemNumber'].iloc[0]\n",
    "    avg_sales_price = group_item_aggr['AvgSalesPrice'].mean()\n",
    "    avg_cost_price = group_item_aggr['AvgCostPrice'].mean()\n",
    "    avg_manual_discount = group_item_aggr['ManualDiscount'].mean()\n",
    "    #avg_consolidation_customer = group_item_aggr['ConsolidationCustomer'].mean()\n",
    "    avg_supplier_leadtime = group_item_aggr['SupplierLeadtime'].mean()\n",
    "    price_category = group_item_aggr['PriceCategory'].iloc[0]\n",
    "    percent_eu = group_item_aggr['PercentEU'].mean()\n",
    "    percent_usa = group_item_aggr['PercentUSA'].mean()\n",
    "    percent_apa = group_item_aggr['PercentAPA'].mean()\n",
    "    VIEngineered = group_item_aggr['VIEngineered'].iloc[0]\n",
    "    SparePartsCategory = group_item_aggr['SparePartsCategory'].iloc[0]\n",
    "    TechnicalClassification = group_item_aggr['TechnicalClassification'].iloc[0]\n",
    "    OEM = group_item_aggr['OEM'].iloc[0]\n",
    "    level1 = group_item_aggr['Level1'].iloc[0]\n",
    "    level2 = group_item_aggr['Level2'].iloc[0]\n",
    "    level3 = group_item_aggr['Level3'].iloc[0]\n",
    "    level4 = group_item_aggr['Level4'].iloc[0]\n",
    "    level5 = group_item_aggr['Level5'].iloc[0]\n",
    "\n",
    "    #avgdaysdifferece = group_item_aggr['MeanDaysDifference'].mean()\n",
    "    #avglatestdaysdifferece = group_item_aggr['MeanLatestDaysDifference'].mean()\n",
    "\n",
    "    previous_row = None\n",
    "\n",
    "    for _, row in group_item_aggr.iterrows():\n",
    "        qty_current_price = row['TotalQuantity']\n",
    "        current_glp = row['GLPSalesUoM']\n",
    "        current_start_time = row['StartTime']\n",
    "        current_end_time = row['EndTime']\n",
    "        current_start_year = row['StartYear']\n",
    "        current_end_year = row['EndYear']\n",
    "        current_inflation = row['AvgInflationIndex']\n",
    "        current_time_active = row['TimeActive']\n",
    "        current_qty_per_day = qty_current_price / current_time_active\n",
    "\n",
    "        \n",
    "        # if we are at the first row (first ever glp) in the item group\n",
    "        if previous_row is None:\n",
    "            qty_previous_price = None\n",
    "            previous_glp = None\n",
    "            previous_inflation = None\n",
    "            previous_time_active = None\n",
    "            time_adj_pe = None\n",
    "            infl_time_adj_pe = None\n",
    "            qty_change = None\n",
    "            glp_change = None\n",
    "            perc_time_adj_qty_change = None\n",
    "            perc_glp_change = None\n",
    "            previous_qty_per_day = None\n",
    "            time_adj_qty_change = None\n",
    "            inflation_ratio = None\n",
    "            adjusted_current_glp = None\n",
    "            perc_inflation_adjusted_glp_change = None\n",
    "        else:\n",
    "            qty_previous_price = previous_row['TotalQuantity']\n",
    "            previous_glp = previous_row['GLPSalesUoM']\n",
    "            previous_inflation = previous_row['AvgInflationIndex']\n",
    "            previous_time_active = previous_row['TimeActive']\n",
    "            previous_qty_per_day = qty_previous_price / previous_time_active\n",
    "\n",
    "            qty_change = qty_current_price - qty_previous_price\n",
    "            time_adj_qty_change = current_qty_per_day - previous_qty_per_day\n",
    "            perc_time_adj_qty_change = time_adj_qty_change / previous_qty_per_day\n",
    "\n",
    "            glp_change = current_glp - previous_glp\n",
    "            perc_glp_change = glp_change / previous_glp\n",
    "\n",
    "            time_adj_pe = perc_time_adj_qty_change / perc_glp_change\n",
    "\n",
    "            if previous_inflation == 0:\n",
    "                previous_inflation = 0.01\n",
    "            \n",
    "            if current_inflation == 0:\n",
    "                current_inflation = 0.01\n",
    "                \n",
    "            inflation_ratio = current_inflation / previous_inflation\n",
    "            adjusted_current_glp = current_glp * inflation_ratio\n",
    "            perc_inflation_adjusted_glp_change = (adjusted_current_glp - previous_glp) / previous_glp\n",
    "            if perc_inflation_adjusted_glp_change == 0:\n",
    "                perc_inflation_adjusted_glp_change = 0.0001\n",
    "            infl_time_adj_pe = perc_time_adj_qty_change / perc_inflation_adjusted_glp_change\n",
    "\n",
    "        pe_data.append({\n",
    "            'ItemNumber': group_item,\n",
    "            'MeanSalesPrice': avg_sales_price,\n",
    "            'MeanCostPrice': avg_cost_price,\n",
    "            'MeanManualDiscount': avg_manual_discount,\n",
    "            #'AvgConsolidationCustomer': avg_consolidation_customer,\n",
    "            'MeanSupplierLeadtime': avg_supplier_leadtime,\n",
    "            'PriceCategory': price_category,\n",
    "            'CurrentGLP': current_glp,\n",
    "            'PreviousGLP': previous_glp,\n",
    "            'CurrentStartTime': current_start_time,\n",
    "            'CurrentEndTime': current_end_time,\n",
    "            'CurrentStartYear': current_start_year,\n",
    "            'CurrentEndYear': current_end_year,\n",
    "            'CurrentInflation': current_inflation,\n",
    "            'PreviousInflation': previous_inflation,\n",
    "            'CurrentTimeActive': current_time_active,\n",
    "            'PreviousTimeActive': previous_time_active,\n",
    "            'CurrentQuantity': qty_current_price,\n",
    "            'PreviousQuantity': qty_previous_price,\n",
    "            'CurrentDailyQty': current_qty_per_day,\n",
    "            'PreviousDailyQty': previous_qty_per_day,\n",
    "            'QuantityChange': qty_change,\n",
    "            'DailyQtyChange': time_adj_qty_change,\n",
    "            'PercDailyQtyChange': perc_time_adj_qty_change,\n",
    "            'GLPChange': glp_change,\n",
    "            'PercGLPChange': perc_glp_change,\n",
    "            'TimeAdjPriceElasticity': time_adj_pe,\n",
    "            'InflationRatio': inflation_ratio,\n",
    "            'DeflatedCurrentGLP': adjusted_current_glp,\n",
    "            'PercInflationAdjustedGLPChange': perc_inflation_adjusted_glp_change,\n",
    "            'InflationTimeAdjPriceElasticity': infl_time_adj_pe,\n",
    "            'PercentEU': percent_eu,\n",
    "            'PercentUSA': percent_usa,\n",
    "            'PercentAPA': percent_apa,\n",
    "            'VIEngineered': VIEngineered,\n",
    "            'SparePartsCategory': SparePartsCategory,\n",
    "            'TechnicalClassification': TechnicalClassification,\n",
    "            'OEM': OEM,\n",
    "            'Level1': level1,\n",
    "            'Level2': level2,\n",
    "            'Level3': level3,\n",
    "            'Level4': level4,\n",
    "            'Level5': level5,\n",
    "            #'MeanDaysDifference': avgdaysdifferece,\n",
    "            #'MeanLatestDaysDifference': avglatestdaysdifferece\n",
    "        })\n",
    "\n",
    "        previous_row = row\n",
    "    \n",
    "    return pd.DataFrame(pe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_pe = item_grouped_aggregated_sorted.groupby(\"ItemNumber\").apply(calculate_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_pe_usa = item_grouped_aggregated_sorted_usa.groupby(\"ItemNumber\").apply(calculate_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_pe_eu_apa = item_grouped_aggregated_sorted_eu_apa.groupby(\"ItemNumber\").apply(calculate_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_pe_eu = item_grouped_aggregated_sorted_eu.groupby(\"ItemNumber\").apply(calculate_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_pe_apa = item_grouped_aggregated_sorted_apa.groupby(\"ItemNumber\").apply(calculate_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe = calculated_pe.reset_index(drop=True)\n",
    "ungrouped_calculated_pe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe_usa = calculated_pe_usa.reset_index(drop=True)\n",
    "ungrouped_calculated_pe_usa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe_eu_apa = calculated_pe_eu_apa.reset_index(drop=True)\n",
    "ungrouped_calculated_pe_eu_apa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe_eu = calculated_pe_eu.reset_index(drop=True)\n",
    "ungrouped_calculated_pe_apa = calculated_pe_apa.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe_eu.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking distributions and clipping\n",
    "Below we check price elasticity distributions. Most values are between -1 and 1, but there are a few outliers, which really influence the discovery process later on. For this reason, we clip the PE values at [-100, 100]. This is equivalent to a 2-3% clip on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select the column of interest (assumed to be floats)\n",
    "data = ungrouped_calculated_pe_eu_apa['InflationTimeAdjPriceElasticity']\n",
    "\n",
    "# Print basic descriptive statistics using pandas' describe()\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Calculate mode (note: mode may return multiple values if there are ties)\n",
    "mode_val = data.mode()\n",
    "print(\"\\nMode:\")\n",
    "print(mode_val)\n",
    "\n",
    "# Also compute the median explicitly (even though it's in describe())\n",
    "median_val = data.median()\n",
    "print(\"\\nMedian:\", median_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_percentile = np.nanpercentile(ungrouped_calculated_pe_usa['InflationTimeAdjPriceElasticity'], 3)\n",
    "print(\"First Percentile Value:\", first_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe_eu_apa['ClippedInflationTimeAdjPriceElasticity'] = ungrouped_calculated_pe_eu_apa['InflationTimeAdjPriceElasticity'].clip(lower=-100, upper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe_eu['ClippedInflationTimeAdjPriceElasticity'] = ungrouped_calculated_pe_eu['InflationTimeAdjPriceElasticity'].clip(lower=-100, upper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe_apa['ClippedInflationTimeAdjPriceElasticity'] = ungrouped_calculated_pe_apa['InflationTimeAdjPriceElasticity'].clip(lower=-100, upper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe_usa['ClippedInflationTimeAdjPriceElasticity'] = ungrouped_calculated_pe_usa['InflationTimeAdjPriceElasticity'].clip(lower=-100, upper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the column of interest (assumed to be floats)\n",
    "data = ungrouped_calculated_pe_eu_apa['ClippedInflationTimeAdjPriceElasticity']\n",
    "\n",
    "# Print basic descriptive statistics using pandas' describe()\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Calculate mode (note: mode may return multiple values if there are ties)\n",
    "mode_val = data.mode()\n",
    "print(\"\\nMode:\")\n",
    "print(mode_val)\n",
    "\n",
    "# Also compute the median explicitly (even though it's in describe())\n",
    "median_val = data.median()\n",
    "print(\"\\nMedian:\", median_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the column of interest (assumed to be floats)\n",
    "data = ungrouped_calculated_pe_usa['ClippedInflationTimeAdjPriceElasticity']\n",
    "\n",
    "# Print basic descriptive statistics using pandas' describe()\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Calculate mode (note: mode may return multiple values if there are ties)\n",
    "mode_val = data.mode()\n",
    "print(\"\\nMode:\")\n",
    "print(mode_val)\n",
    "\n",
    "# Also compute the median explicitly (even though it's in describe())\n",
    "median_val = data.median()\n",
    "print(\"\\nMedian:\", median_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the column of interest (assumed to be floats)\n",
    "data = ungrouped_calculated_pe_eu_apa['TimeAdjPriceElasticity']\n",
    "\n",
    "# Print basic descriptive statistics using pandas' describe()\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Calculate mode (note: mode may return multiple values if there are ties)\n",
    "mode_val = data.mode()\n",
    "print(\"\\nMode:\")\n",
    "print(mode_val)\n",
    "\n",
    "# Also compute the median explicitly (even though it's in describe())\n",
    "median_val = data.median()\n",
    "print(\"\\nMedian:\", median_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate to one row per product\n",
    "Below, we get rid of redundant information. We have one row per product, containing all the features, and arrays where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ItemNumber and aggregate the TimeAdjPriceElasticity and InflationTimeAdjPriceElasticity into arrays\n",
    "aggregated_df = ungrouped_calculated_pe.groupby('ItemNumber').agg({\n",
    "    'TimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'InflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'DeflatedCurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'CurrentQuantity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentDailyQty': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartYear': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndYear': lambda x: x.dropna().tolist(),\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ItemNumber and aggregate the TimeAdjPriceElasticity and InflationTimeAdjPriceElasticity into arrays\n",
    "aggregated_df_usa = ungrouped_calculated_pe_usa.groupby('ItemNumber').agg({\n",
    "    'TimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'InflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'ClippedInflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'DeflatedCurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'CurrentQuantity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentDailyQty': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartYear': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndYear': lambda x: x.dropna().tolist(),\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_usa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ItemNumber and aggregate the TimeAdjPriceElasticity and InflationTimeAdjPriceElasticity into arrays\n",
    "aggregated_df_eu_apa = ungrouped_calculated_pe_eu_apa.groupby('ItemNumber').agg({\n",
    "    'TimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'InflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'ClippedInflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'DeflatedCurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'CurrentQuantity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentDailyQty': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartYear': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndYear': lambda x: x.dropna().tolist(),\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu_apa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ItemNumber and aggregate the TimeAdjPriceElasticity and InflationTimeAdjPriceElasticity into arrays\n",
    "aggregated_df_eu = ungrouped_calculated_pe_eu.groupby('ItemNumber').agg({\n",
    "    'TimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'InflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'ClippedInflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'DeflatedCurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'CurrentQuantity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentDailyQty': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartYear': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndYear': lambda x: x.dropna().tolist(),\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ItemNumber and aggregate the TimeAdjPriceElasticity and InflationTimeAdjPriceElasticity into arrays\n",
    "aggregated_df_apa = ungrouped_calculated_pe_apa.groupby('ItemNumber').agg({\n",
    "    'TimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'InflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'ClippedInflationTimeAdjPriceElasticity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'DeflatedCurrentGLP': lambda x: x.dropna().tolist(),\n",
    "    'CurrentQuantity': lambda x: x.dropna().tolist(),\n",
    "    'CurrentDailyQty': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndTime': lambda x: x.dropna().tolist(),\n",
    "    'CurrentStartYear': lambda x: x.dropna().tolist(),\n",
    "    'CurrentEndYear': lambda x: x.dropna().tolist(),\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_apa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_calculated_pe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to merge from calculated_pe\n",
    "columns_to_merge = ['ItemNumber', 'MeanSalesPrice', 'MeanCostPrice', 'MeanManualDiscount',  'MeanSupplierLeadtime', 'PriceCategory', 'PercentEU', 'PercentUSA', 'PercentAPA', 'VIEngineered', 'SparePartsCategory', 'TechnicalClassification', 'OEM', 'Level1', 'Level2', 'Level3', 'Level4', 'Level5']\n",
    "\n",
    "# Merge the selected columns into aggregated_df\n",
    "aggregated_df = pd.merge(aggregated_df, ungrouped_calculated_pe[columns_to_merge].drop_duplicates(subset=['ItemNumber']), on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_usa = pd.merge(aggregated_df_usa, ungrouped_calculated_pe_usa[columns_to_merge].drop_duplicates(subset=['ItemNumber']), on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu_apa = pd.merge(aggregated_df_eu_apa, ungrouped_calculated_pe_eu_apa[columns_to_merge].drop_duplicates(subset=['ItemNumber']), on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu = pd.merge(aggregated_df_eu, ungrouped_calculated_pe_eu[columns_to_merge].drop_duplicates(subset=['ItemNumber']), on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_apa = pd.merge(aggregated_df_apa, ungrouped_calculated_pe_apa[columns_to_merge].drop_duplicates(subset=['ItemNumber']), on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_usa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu_apa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df['MeanProfit'] = np.where((aggregated_df['MeanSalesPrice'] == 0) | (aggregated_df['MeanCostPrice'] == 0), np.nan, aggregated_df['MeanSalesPrice'] - aggregated_df['MeanCostPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_usa['MeanProfit'] = np.where((aggregated_df_usa['MeanSalesPrice'] == 0) | (aggregated_df_usa['MeanCostPrice'] == 0), np.nan, aggregated_df_usa['MeanSalesPrice'] - aggregated_df_usa['MeanCostPrice'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu_apa['MeanProfit'] = np.where((aggregated_df_eu_apa['MeanSalesPrice'] == 0) | (aggregated_df_eu_apa['MeanCostPrice'] == 0), np.nan, aggregated_df_eu_apa['MeanSalesPrice'] - aggregated_df_eu_apa['MeanCostPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu['MeanProfit'] = np.where((aggregated_df_eu['MeanSalesPrice'] == 0) | (aggregated_df_eu['MeanCostPrice'] == 0), np.nan, aggregated_df_eu['MeanSalesPrice'] - aggregated_df_eu['MeanCostPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_apa['MeanProfit'] = np.where((aggregated_df_apa['MeanSalesPrice'] == 0) | (aggregated_df_apa['MeanCostPrice'] == 0), np.nan, aggregated_df_apa['MeanSalesPrice'] - aggregated_df_apa['MeanCostPrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute trends for categorical target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_zero_to_neg_transition(array):\n",
    "    \"\"\"\n",
    "    Checks if the array has a transition from zero to negative values\n",
    "    without returning to zero or positive.\n",
    "    \"\"\"\n",
    "    seen_negative = False  # Tracks if we have encountered negative numbers\n",
    "    for value in array:\n",
    "        if value == 0:\n",
    "            if seen_negative:\n",
    "                return False  # Encountered a zero after a negative\n",
    "        elif value < 0:\n",
    "            seen_negative = True  # Start tracking negative numbers\n",
    "        else:\n",
    "            return False  # Positive values are not allowed\n",
    "    # Ensure the array has at least one zero and one negative number\n",
    "    return seen_negative and array[0] == 0\n",
    "\n",
    "def has_zero_to_pos_transition(array):\n",
    "    \"\"\"\n",
    "    Checks if the array has a transition from zero to positive values\n",
    "    without returning to zero or negative.\n",
    "    \"\"\"\n",
    "    seen_positive = False  # Tracks if we have encountered positive numbers\n",
    "    for value in array:\n",
    "        if value == 0:\n",
    "            if seen_positive:\n",
    "                return False  # Encountered a zero after a positive\n",
    "        elif value > 0:\n",
    "            seen_positive = True  # Start tracking positive numbers\n",
    "        else:\n",
    "            return False  # Negative values are not allowed\n",
    "    # Ensure the array has at least one zero and one positive number\n",
    "    return seen_positive and array[0] == 0\n",
    "def has_negative_to_positive_transition(array):\n",
    "    \"\"\"\n",
    "    Checks if the array has a transition from negative to positive values\n",
    "    without returning to negative.\n",
    "    \"\"\"\n",
    "    seen_positive = False  # Tracks if we have encountered positive numbers\n",
    "    for value in array:\n",
    "        if value < 0:\n",
    "            if seen_positive:\n",
    "                return False  # Encountered a negative number after a positive\n",
    "        elif value > 0:\n",
    "            seen_positive = True  # Start tracking positive numbers\n",
    "        else:\n",
    "            return False  # Zero is not allowed\n",
    "    # Ensure the array has at least one negative and one positive number\n",
    "    return seen_positive and array[0] < 0\n",
    "\n",
    "def has_positive_to_negative_transition(array):\n",
    "    \"\"\"\n",
    "    Checks if the array has a transition from negative to positive values\n",
    "    without returning to negative.\n",
    "    \"\"\"\n",
    "    seen_negative = False  # Tracks if we have encountered negative numbers\n",
    "    for value in array:\n",
    "        if value > 0:\n",
    "            if seen_negative:\n",
    "                return False  # Encountered a positive number after a negative\n",
    "        elif value < 0:\n",
    "            seen_negative = True  # Start tracking negative numbers\n",
    "        else:\n",
    "            return False  # Zero is not allowed\n",
    "    # Ensure the array has at least one positive and one negative number\n",
    "    return seen_negative and array[0] > 0\n",
    "\n",
    "def is_positive_then_zero(array):\n",
    "    \"\"\"\n",
    "    Checks if the array starts with n positive numbers followed by m zeroes (n > 0, m > 0).\n",
    "    The array must start with at least one positive number and transition to zeroes without returning to positive or encountering negative values.\n",
    "    \"\"\"\n",
    "    seen_zero = False  # Tracks if we have encountered zeroes\n",
    "    for value in array:\n",
    "        if value > 0:\n",
    "            if seen_zero:\n",
    "                return False  # Encountered a positive number after zeroes\n",
    "        elif value == 0:\n",
    "            seen_zero = True  # Start tracking zeroes\n",
    "        else:\n",
    "            return False  # Negative values are not allowed\n",
    "    # Ensure the array has at least one positive number and one zero\n",
    "    return seen_zero and array[0] > 0\n",
    "\n",
    "def is_negative_then_zero(array):\n",
    "\n",
    "    seen_zero = False  # Tracks if we have encountered zeroes\n",
    "    for value in array:\n",
    "        if value < 0:\n",
    "            if seen_zero:\n",
    "                return False  # Encountered a positive number after zeroes\n",
    "        elif value == 0:\n",
    "            seen_zero = True  # Start tracking zeroes\n",
    "        else:\n",
    "            return False  # Negative values are not allowed\n",
    "    # Ensure the array has at least one positive number and one zero\n",
    "    return seen_zero and array[0] < 0\n",
    "# Define the trend function for negative to zero transition\n",
    "def detect_price_elasticity_trend(row):\n",
    "    if is_negative_then_zero(row['TimeAdjPriceElasticity']):\n",
    "        return \"neg to zero\"\n",
    "    if is_positive_then_zero(row['TimeAdjPriceElasticity']):\n",
    "        return \"pos to zero\"\n",
    "    if has_positive_to_negative_transition(row['TimeAdjPriceElasticity']):\n",
    "        return \"pos to neg\"\n",
    "    if has_negative_to_positive_transition(row['TimeAdjPriceElasticity']):\n",
    "        return \"neg to pos\"\n",
    "    if has_zero_to_neg_transition(row['TimeAdjPriceElasticity']):\n",
    "        return \"zero to neg\"\n",
    "    if has_zero_to_pos_transition(row['TimeAdjPriceElasticity']):\n",
    "        return \"zero to pos\"\n",
    "    if all(e > 0 for e in row['TimeAdjPriceElasticity']):\n",
    "        return \"positive\"\n",
    "    if all(e < 0 for e in row['TimeAdjPriceElasticity']):\n",
    "        return \"negative\"\n",
    "    if all(e == 0 for e in row['TimeAdjPriceElasticity']):\n",
    "        return \"zero\"\n",
    "    return \"erratic\"  # Default for rows not matching the condition\n",
    "\n",
    "# Add the PriceElasticityTrend column\n",
    "aggregated_df['PriceElasticityTrendTimeAdj'] = aggregated_df.apply(detect_price_elasticity_trend, axis=1)\n",
    "# Define the trend function for negative to zero transition\n",
    "def detect_price_elasticity_trend_inflation(row):\n",
    "    if is_negative_then_zero(row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"neg to zero\"\n",
    "    if is_positive_then_zero(row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"pos to zero\"\n",
    "    if has_positive_to_negative_transition(row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"pos to neg\"\n",
    "    if has_negative_to_positive_transition(row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"neg to pos\"\n",
    "    if has_zero_to_neg_transition(row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"zero to neg\"\n",
    "    if has_zero_to_pos_transition(row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"zero to pos\"\n",
    "    if all(e > 0 for e in row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"positive\"\n",
    "    if all(e < 0 for e in row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"negative\"\n",
    "    if all(e == 0 for e in row['InflationTimeAdjPriceElasticity']):\n",
    "        return \"zero\"\n",
    "    return \"erratic\"  # Default for rows not matching the condition\n",
    "\n",
    "# Add the PriceElasticityTrend column\n",
    "aggregated_df['InflPriceElasticityTrendTimeAdj'] = aggregated_df.apply(detect_price_elasticity_trend_inflation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_usa['PriceElasticityTrendTimeAdj'] = aggregated_df_usa.apply(detect_price_elasticity_trend, axis=1)\n",
    "aggregated_df_usa['InflPriceElasticityTrendTimeAdj'] = aggregated_df_usa.apply(detect_price_elasticity_trend_inflation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu_apa['PriceElasticityTrendTimeAdj'] = aggregated_df_eu_apa.apply(detect_price_elasticity_trend, axis=1)\n",
    "aggregated_df_eu_apa['InflPriceElasticityTrendTimeAdj'] = aggregated_df_eu_apa.apply(detect_price_elasticity_trend_inflation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu['PriceElasticityTrendTimeAdj'] = aggregated_df_eu.apply(detect_price_elasticity_trend, axis=1)\n",
    "aggregated_df_eu['InflPriceElasticityTrendTimeAdj'] = aggregated_df_eu.apply(detect_price_elasticity_trend_inflation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_apa['PriceElasticityTrendTimeAdj'] = aggregated_df_apa.apply(detect_price_elasticity_trend, axis=1)\n",
    "aggregated_df_apa['InflPriceElasticityTrendTimeAdj'] = aggregated_df_apa.apply(detect_price_elasticity_trend_inflation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aggregated_df_eu_apa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_usa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df['PriceElasticityTrendTimeAdj'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df['InflPriceElasticityTrendTimeAdj'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_usa['InflPriceElasticityTrendTimeAdj'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_eu_apa['InflPriceElasticityTrendTimeAdj'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_order_qty = df_wide_data.groupby('ItemNumber')['OrderQuantity'].mean().reset_index()\n",
    "mean_order_qty.columns = ['ItemNumber', 'MeanOrderQty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_order_qty_usa = df_wide_data_usa.groupby('ItemNumber')['OrderQuantity'].mean().reset_index()\n",
    "mean_order_qty_usa.columns = ['ItemNumber', 'MeanOrderQty']\n",
    "aggregated_df_usa = pd.merge(aggregated_df_usa, mean_order_qty_usa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_order_qty_eu_apa = df_wide_data_eu_apa.groupby('ItemNumber')['OrderQuantity'].mean().reset_index()\n",
    "mean_order_qty_eu_apa.columns = ['ItemNumber', 'MeanOrderQty']\n",
    "aggregated_df_eu_apa = pd.merge(aggregated_df_eu_apa, mean_order_qty_eu_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_order_qty_eu = df_wide_data_eu.groupby('ItemNumber')['OrderQuantity'].mean().reset_index()\n",
    "mean_order_qty_eu.columns = ['ItemNumber', 'MeanOrderQty']\n",
    "aggregated_df_eu = pd.merge(aggregated_df_eu, mean_order_qty_eu, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_order_qty_apa = df_wide_data_apa.groupby('ItemNumber')['OrderQuantity'].mean().reset_index()\n",
    "mean_order_qty_apa.columns = ['ItemNumber', 'MeanOrderQty']\n",
    "aggregated_df_apa = pd.merge(aggregated_df_apa, mean_order_qty_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = pd.merge(aggregated_df, mean_order_qty, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_discount = df_wide_data.groupby('ItemNumber')['Discount'].mean().reset_index()\n",
    "mean_discount.columns = ['ItemNumber', 'Discount']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new = pd.merge(aggregated_df, mean_discount, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_discount_usa = df_wide_data_usa.groupby('ItemNumber')['Discount'].mean().reset_index()\n",
    "mean_discount_usa.columns = ['ItemNumber', 'Discount']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_usa = pd.merge(aggregated_df_usa, mean_discount_usa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_discount_eu_apa = df_wide_data_eu_apa.groupby('ItemNumber')['Discount'].mean().reset_index()\n",
    "mean_discount_eu_apa.columns = ['ItemNumber', 'Discount']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_eu_apa = pd.merge(aggregated_df_eu_apa, mean_discount_eu_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_discount_eu = df_wide_data_eu.groupby('ItemNumber')['Discount'].mean().reset_index()\n",
    "mean_discount_eu.columns = ['ItemNumber', 'Discount']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_eu = pd.merge(aggregated_df_eu, mean_discount_eu, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_discount_apa = df_wide_data_apa.groupby('ItemNumber')['Discount'].mean().reset_index()\n",
    "mean_discount_apa.columns = ['ItemNumber', 'Discount']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_apa = pd.merge(aggregated_df_apa, mean_discount_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_in_order = df_wide_data.groupby(['OrderDate', 'CustomerSoldTo'])['ItemNumber'].nunique().reset_index()\n",
    "df_wide_data_items_in_order = df_wide_data.merge(\n",
    "    items_in_order,\n",
    "    on=['OrderDate', 'CustomerSoldTo'],\n",
    "    how='left',\n",
    "    suffixes=('', '_ItemsInOrder')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order.rename(columns={'ItemNumber_ItemsInOrder': 'ItemsInOrder'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_in_order_usa = df_wide_data_usa.groupby(['OrderDate', 'CustomerSoldTo'])['ItemNumber'].nunique().reset_index()\n",
    "df_wide_data_items_in_order_usa = df_wide_data_usa.merge(\n",
    "    items_in_order_usa,\n",
    "    on=['OrderDate', 'CustomerSoldTo'],\n",
    "    how='left',\n",
    "    suffixes=('', '_ItemsInOrder')\n",
    ")\n",
    "df_wide_data_items_in_order_usa.rename(columns={'ItemNumber_ItemsInOrder': 'ItemsInOrder'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_in_order_eu_apa = df_wide_data_eu_apa.groupby(['OrderDate', 'CustomerSoldTo'])['ItemNumber'].nunique().reset_index()\n",
    "df_wide_data_items_in_order_eu_apa = df_wide_data_eu_apa.merge(\n",
    "    items_in_order_eu_apa,\n",
    "    on=['OrderDate', 'CustomerSoldTo'],\n",
    "    how='left',\n",
    "    suffixes=('', '_ItemsInOrder')\n",
    ")\n",
    "df_wide_data_items_in_order_eu_apa.rename(columns={'ItemNumber_ItemsInOrder': 'ItemsInOrder'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_in_order_eu = df_wide_data_eu.groupby(['OrderDate', 'CustomerSoldTo'])['ItemNumber'].nunique().reset_index()\n",
    "df_wide_data_items_in_order_eu = df_wide_data_eu.merge(\n",
    "    items_in_order_eu,\n",
    "    on=['OrderDate', 'CustomerSoldTo'],\n",
    "    how='left',\n",
    "    suffixes=('', '_ItemsInOrder')\n",
    ")\n",
    "df_wide_data_items_in_order_eu.rename(columns={'ItemNumber_ItemsInOrder': 'ItemsInOrder'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_in_order_apa = df_wide_data_apa.groupby(['OrderDate', 'CustomerSoldTo'])['ItemNumber'].nunique().reset_index()\n",
    "df_wide_data_items_in_order_apa = df_wide_data_apa.merge(\n",
    "    items_in_order_apa,\n",
    "    on=['OrderDate', 'CustomerSoldTo'],\n",
    "    how='left',\n",
    "    suffixes=('', '_ItemsInOrder')\n",
    ")\n",
    "df_wide_data_items_in_order_apa.rename(columns={'ItemNumber_ItemsInOrder': 'ItemsInOrder'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_items_in_order = df_wide_data_items_in_order.groupby('ItemNumber')['ItemsInOrder'].mean().reset_index()\n",
    "mean_items_in_order.columns = ['ItemNumber', 'ItemsInOrder']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new = pd.merge(aggregated_df_new, mean_items_in_order, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_items_in_order_usa = df_wide_data_items_in_order_usa.groupby('ItemNumber')['ItemsInOrder'].mean().reset_index()\n",
    "mean_items_in_order_usa.columns = ['ItemNumber', 'ItemsInOrder']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_usa = pd.merge(aggregated_df_new_usa, mean_items_in_order_usa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_items_in_order_eu_apa = df_wide_data_items_in_order_eu_apa.groupby('ItemNumber')['ItemsInOrder'].mean().reset_index()\n",
    "mean_items_in_order_eu_apa.columns = ['ItemNumber', 'ItemsInOrder']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_eu_apa = pd.merge(aggregated_df_new_eu_apa, mean_items_in_order_eu_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_items_in_order_eu = df_wide_data_items_in_order_eu.groupby('ItemNumber')['ItemsInOrder'].mean().reset_index()\n",
    "mean_items_in_order_eu.columns = ['ItemNumber', 'ItemsInOrder']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_eu = pd.merge(aggregated_df_new_eu, mean_items_in_order_eu, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_items_in_order_apa = df_wide_data_items_in_order_apa.groupby('ItemNumber')['ItemsInOrder'].mean().reset_index()\n",
    "mean_items_in_order_apa.columns = ['ItemNumber', 'ItemsInOrder']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_apa = pd.merge(aggregated_df_new_apa, mean_items_in_order_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_usa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_eu_apa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame and select the top 100 rows\n",
    "#price_input_error = df_wide_data_items_in_order[df_wide_data_items_in_order['PriceSalesUoMEUR'] >= 100000].head(100)\n",
    "\n",
    "# Save the filtered DataFrame to an Excel file\n",
    "#price_input_error.to_excel('price_input_error.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order['DiscountPercent'] = (df_wide_data_items_in_order['GLPSalesUoM'] - df_wide_data_items_in_order['PriceSalesUoMEUR']) / df_wide_data_items_in_order['GLPSalesUoM']\n",
    "\n",
    "df_wide_data_items_in_order.loc[df_wide_data_items_in_order['GLPSalesUoM'] == 0, 'DiscountPercent'] = 0\n",
    "df_wide_data_items_in_order.loc[df_wide_data_items_in_order['PriceSalesUoMEUR'] == 0, 'DiscountPercent'] = 0\n",
    "\n",
    "mean_discount_percent = df_wide_data_items_in_order.groupby('ItemNumber')['DiscountPercent'].mean().reset_index()\n",
    "mean_discount_percent.columns = ['ItemNumber', 'DiscountPercent']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new = pd.merge(aggregated_df_new, mean_discount_percent, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_usa['DiscountPercent'] = (df_wide_data_items_in_order_usa['GLPSalesUoM'] - df_wide_data_items_in_order_usa['PriceSalesUoMEUR']) / df_wide_data_items_in_order_usa['GLPSalesUoM']\n",
    "\n",
    "df_wide_data_items_in_order_usa.loc[df_wide_data_items_in_order_usa['GLPSalesUoM'] == 0, 'DiscountPercent'] = 0\n",
    "df_wide_data_items_in_order_usa.loc[df_wide_data_items_in_order_usa['PriceSalesUoMEUR'] == 0, 'DiscountPercent'] = 0\n",
    "\n",
    "mean_discount_percent_usa = df_wide_data_items_in_order_usa.groupby('ItemNumber')['DiscountPercent'].mean().reset_index()\n",
    "mean_discount_percent_usa.columns = ['ItemNumber', 'DiscountPercent']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_usa = pd.merge(aggregated_df_new_usa, mean_discount_percent_usa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_eu_apa['DiscountPercent'] = (df_wide_data_items_in_order_eu_apa['GLPSalesUoM'] - df_wide_data_items_in_order_eu_apa['PriceSalesUoMEUR']) / df_wide_data_items_in_order_eu_apa['GLPSalesUoM']\n",
    "\n",
    "df_wide_data_items_in_order_eu_apa.loc[df_wide_data_items_in_order_eu_apa['GLPSalesUoM'] == 0, 'DiscountPercent'] = 0\n",
    "df_wide_data_items_in_order_eu_apa.loc[df_wide_data_items_in_order_eu_apa['PriceSalesUoMEUR'] == 0, 'DiscountPercent'] = 0\n",
    "\n",
    "mean_discount_percent_eu_apa = df_wide_data_items_in_order_eu_apa.groupby('ItemNumber')['DiscountPercent'].mean().reset_index()\n",
    "mean_discount_percent_eu_apa.columns = ['ItemNumber', 'DiscountPercent']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_eu_apa = pd.merge(aggregated_df_new_eu_apa, mean_discount_percent_eu_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_eu['DiscountPercent'] = (df_wide_data_items_in_order_eu['GLPSalesUoM'] - df_wide_data_items_in_order_eu['PriceSalesUoMEUR']) / df_wide_data_items_in_order_eu['GLPSalesUoM']\n",
    "\n",
    "df_wide_data_items_in_order_eu.loc[df_wide_data_items_in_order_eu['GLPSalesUoM'] == 0, 'DiscountPercent'] = 0\n",
    "df_wide_data_items_in_order_eu.loc[df_wide_data_items_in_order_eu['PriceSalesUoMEUR'] == 0, 'DiscountPercent'] = 0\n",
    "\n",
    "mean_discount_percent_eu = df_wide_data_items_in_order_eu.groupby('ItemNumber')['DiscountPercent'].mean().reset_index()\n",
    "mean_discount_percent_eu.columns = ['ItemNumber', 'DiscountPercent']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_eu = pd.merge(aggregated_df_new_eu, mean_discount_percent_eu, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_apa['DiscountPercent'] = (df_wide_data_items_in_order_apa['GLPSalesUoM'] - df_wide_data_items_in_order_apa['PriceSalesUoMEUR']) / df_wide_data_items_in_order_apa['GLPSalesUoM']\n",
    "\n",
    "df_wide_data_items_in_order_apa.loc[df_wide_data_items_in_order_apa['GLPSalesUoM'] == 0, 'DiscountPercent'] = 0\n",
    "df_wide_data_items_in_order_apa.loc[df_wide_data_items_in_order_apa['PriceSalesUoMEUR'] == 0, 'DiscountPercent'] = 0\n",
    "\n",
    "mean_discount_percent_apa = df_wide_data_items_in_order_apa.groupby('ItemNumber')['DiscountPercent'].mean().reset_index()\n",
    "mean_discount_percent_apa.columns = ['ItemNumber', 'DiscountPercent']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_apa = pd.merge(aggregated_df_new_apa, mean_discount_percent_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockedItems = pd.read_excel(\"Stocked Items.xlsx\", sheet_name = \"S4Export_12\")\n",
    "# Create a set of stocked item codes for faster lookup\n",
    "stocked_item_codes = set(StockedItems['Code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the 'stocked' column to merged_data\n",
    "aggregated_df_new['Stocked'] = aggregated_df_new['ItemNumber'].apply(lambda x: 1 if x in stocked_item_codes else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_eu_apa['Stocked'] = aggregated_df_new_eu_apa['ItemNumber'].apply(lambda x: 1 if x in stocked_item_codes else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_eu['Stocked'] = aggregated_df_new_eu['ItemNumber'].apply(lambda x: 1 if x in stocked_item_codes else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_apa['Stocked'] = aggregated_df_new_apa['ItemNumber'].apply(lambda x: 1 if x in stocked_item_codes else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_usa['Stocked'] = aggregated_df_new_usa['ItemNumber'].apply(lambda x: 1 if x in stocked_item_codes else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aggregated_df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aggregated_df_new_usa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aggregated_df_new_eu_apa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows where PriceElasticityTrendTimeAdj is not equal to InflPriceElasticityTrendTimeAdj\n",
    "mismatch_count = aggregated_df_new[aggregated_df_new['PriceElasticityTrendTimeAdj'] != aggregated_df_new['InflPriceElasticityTrendTimeAdj']].shape[0]\n",
    "print(f'There are {mismatch_count} rows where PriceElasticityTrendTimeAdj is not equal to InflPriceElasticityTrendTimeAdj.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample to 8 elasticities per product\n",
    "Here we resample to 8 elasticities per product, roughly corresponding to 2 price changes a year over 4 years. If you want to run on a different period, changed fixed length to the number of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_array(arr, fixed_length):\n",
    "    \"\"\"\n",
    "    Resample a 1D numpy array to a fixed length using linear interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        arr (array-like): Original array of numeric values.\n",
    "        fixed_length (int): Desired length of the resampled array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resampled array of length 'fixed_length'.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    original_length = len(arr)\n",
    "\n",
    "    if original_length == fixed_length:\n",
    "        return arr\n",
    "\n",
    "    # Create original and new equally spaced indices (scaled from 0 to 1)\n",
    "    original_indices = np.linspace(0, 1, original_length)\n",
    "    target_indices = np.linspace(0, 1, fixed_length)\n",
    "\n",
    "    # Use linear interpolation to compute resampled values\n",
    "    resampled = np.interp(target_indices, original_indices, arr)\n",
    "\n",
    "    return resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fixed length you want\n",
    "fixed_length = 8\n",
    "\n",
    "# Use your resample_array function\n",
    "aggregated_df_new['ResampledInflationPE'] = aggregated_df_new['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_usa['ResampledInflationPE'] = aggregated_df_new_usa['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu_apa['ResampledInflationPE'] = aggregated_df_new_eu_apa['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "# Use your resample_array function\n",
    "aggregated_df_new['ResampledStandardPE'] = aggregated_df_new['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_usa['ResampledStandardPE'] = aggregated_df_new_usa['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu_apa['ResampledStandardPE'] = aggregated_df_new_eu_apa['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "# Define the fixed length you want\n",
    "fixed_length = 8\n",
    "\n",
    "\n",
    "aggregated_df_new_usa['ClippedResampledInflationPE'] = aggregated_df_new_usa['ClippedInflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu_apa['ClippedResampledInflationPE'] = aggregated_df_new_eu_apa['ClippedInflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu['ResampledInflationPE'] = aggregated_df_new_eu['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu['ResampledStandardPE'] = aggregated_df_new_eu['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu['ClippedResampledInflationPE'] = aggregated_df_new_eu['ClippedInflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_apa['ResampledInflationPE'] = aggregated_df_new_apa['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_apa['ResampledStandardPE'] = aggregated_df_new_apa['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_apa['ClippedResampledInflationPE'] = aggregated_df_new_apa['ClippedInflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fixed length you want\n",
    "fixed_length = 8\n",
    "\n",
    "# Use your resample_array function\n",
    "aggregated_df_new['8ResampledInflationPE'] = aggregated_df_new['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_usa['8ResampledInflationPE'] = aggregated_df_new_usa['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu_apa['8ResampledInflationPE'] = aggregated_df_new_eu_apa['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "# Use your resample_array function\n",
    "aggregated_df_new['8ResampledStandardPE'] = aggregated_df_new['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_usa['8ResampledStandardPE'] = aggregated_df_new_usa['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu_apa['8ResampledStandardPE'] = aggregated_df_new_eu_apa['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "\n",
    "\n",
    "\n",
    "aggregated_df_new_usa['8ClippedResampledInflationPE'] = aggregated_df_new_usa['ClippedInflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu_apa['8ClippedResampledInflationPE'] = aggregated_df_new_eu_apa['ClippedInflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu['8ResampledInflationPE'] = aggregated_df_new_eu['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu['8ResampledStandardPE'] = aggregated_df_new_eu['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_eu['8ClippedResampledInflationPE'] = aggregated_df_new_eu['ClippedInflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_apa['8ResampledInflationPE'] = aggregated_df_new_apa['InflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_apa['8ResampledStandardPE'] = aggregated_df_new_apa['TimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))\n",
    "aggregated_df_new_apa['8ClippedResampledInflationPE'] = aggregated_df_new_apa['ClippedInflationTimeAdjPriceElasticity'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), fixed_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add consolidation customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load consolidation customers from the specified Excel file\n",
    "consolidation_customers = pd.read_excel(\"ConsolidationCustomers.xlsx\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "\n",
    "consolidation_customers.rename(columns={'Ship to:': 'ShipTo'}, inplace=True)\n",
    "\n",
    "consolidation_customers.rename(columns={'Consignee ': 'Consignee'}, inplace=True)\n",
    "consolidation_customers['ShipTo'] = consolidation_customers['ShipTo'].astype(\"string\")\n",
    "consolidation_customers['Consignee'] = consolidation_customers['Consignee'].astype(\"string\")\n",
    "consolidation_customers['City'] = consolidation_customers['City'].astype(\"string\")\n",
    "consolidation_customers['Country'] = consolidation_customers['Country'].astype(\"string\")\n",
    "def split_customer_sold_to(row):\n",
    "    \"\"\"\n",
    "    Splits 'CustomerSoldTo' into CustomerNumber and CustomerName.\n",
    "    - Extracts the first numeric part as CustomerNumber.\n",
    "    - The remainder of the string is treated as CustomerName.\n",
    "    - If no number or name exists, returns None for that part.\n",
    "    \"\"\"\n",
    "    if pd.isnull(row):\n",
    "        return pd.Series({'CustomerNumber': None, 'CustomerName': None})\n",
    "    \n",
    "    parts = row.split(' - ', 1)  # Split at first occurrence\n",
    "    if len(parts) == 2:\n",
    "        number_part, name_part = parts[0].strip(), parts[1].strip()\n",
    "        return pd.Series({'CustomerNumber': number_part, 'CustomerName': name_part})\n",
    "    else:\n",
    "        # Handle cases without exactly one ' - '\n",
    "        number_part = ''.join(filter(str.isdigit, parts[0])).strip()\n",
    "        name_part = parts[0].strip() if number_part == '' else None\n",
    "        return pd.Series({'CustomerNumber': number_part if number_part else None,\n",
    "                          'CustomerName': name_part if len(parts) > 1 else name_part})\n",
    "df_wide_data_items_in_order_copy = df_wide_data_items_in_order.copy()\n",
    "# Apply function:\n",
    "df_wide_data_items_in_order_copy[['CustomerNumber', 'CustomerName']] = df_wide_data_items_in_order_copy['CustomerSoldTo'].apply(split_customer_sold_to)\n",
    "\n",
    "def split_ship_to(row):\n",
    "    \"\"\"\n",
    "    Splits 'CustomerSoldTo' into CustomerNumber and CustomerName.\n",
    "    - Extracts the first numeric part as CustomerNumber.\n",
    "    - The remainder of the string is treated as CustomerName.\n",
    "    - If no number or name exists, returns None for that part.\n",
    "    \"\"\"\n",
    "    if pd.isnull(row):\n",
    "        return pd.Series({'ShipNumber': None, 'ShipName': None})\n",
    "    \n",
    "    parts = row.split(' - ', 1)  # Split at first occurrence\n",
    "    if len(parts) == 2:\n",
    "        number_part, name_part = parts[0].strip(), parts[1].strip()\n",
    "        return pd.Series({'ShipNumber': number_part, 'ShipName': name_part})\n",
    "    else:\n",
    "        # Handle cases without exactly one ' - '\n",
    "        number_part = ''.join(filter(str.isdigit, parts[0])).strip()\n",
    "        name_part = parts[0].strip() if number_part == '' else None\n",
    "        return pd.Series({'ShipNumber': number_part if number_part else None,\n",
    "                          'ShipName': name_part if len(parts) > 1 else name_part})\n",
    "    \n",
    "df_wide_data_items_in_order_copy[['ShipNumber', 'ShipName']] = df_wide_data_items_in_order_copy['ShipTo'].apply(split_ship_to)\n",
    "df_wide_data_items_in_order_copy['ShipNumber'] = df_wide_data_items_in_order_copy['ShipNumber'].str.replace(' ', '', regex=False)\n",
    "df_wide_data_items_in_order_copy['CustomerNumber'] = df_wide_data_items_in_order_copy['CustomerNumber'].str.replace(' ', '', regex=False)\n",
    "consolidation_customers['ShipTo'] = consolidation_customers['ShipTo'].str.replace(' ', '', regex=False)\n",
    "\n",
    "consignee_list = consolidation_customers['Consignee'].dropna().unique().tolist()\n",
    "\n",
    "# Define a helper function to match CustomerName with Consignee strings\n",
    "def consignee_match(customer_name, consignee_list):\n",
    "    if pd.isnull(customer_name):\n",
    "        return False\n",
    "    for consignee in consignee_list:\n",
    "        if consignee.lower() in customer_name.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Apply conditions\n",
    "df_wide_data_items_in_order_copy['ConsolidationCustomer'] = np.where(\n",
    "    df_wide_data_items_in_order_copy['ShipNumber'].isin(consolidation_customers['ShipTo']) |\n",
    "    df_wide_data_items_in_order_copy['CustomerNumber'].isin(consolidation_customers['ShipTo']) \n",
    "    #| df_wide_data_items_in_order_copy['CustomerName'].apply(lambda x: consignee_match(x, consolidation_customers['Consignee '])) |\n",
    "    #df_wide_data_items_in_order_copy['ShipName'].apply(lambda x: consignee_match(x, consolidation_customers['Consignee ']))\n",
    "    \n",
    "    , 1, 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_copy['ConsolidationCustomer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order = df_wide_data_items_in_order_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_eu_apa = df_wide_data_items_in_order[df_wide_data_items_in_order['GeographicRegion'] != 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_eu = df_wide_data_items_in_order[df_wide_data_items_in_order['GeographicRegion'] == 'EUR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_apa = df_wide_data_items_in_order[df_wide_data_items_in_order['GeographicRegion'] == 'APA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_data_items_in_order_usa = df_wide_data_items_in_order[df_wide_data_items_in_order['GeographicRegion'] == 'USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_consolidation_cust_eu_apa = df_wide_data_items_in_order_eu_apa.groupby('ItemNumber')['ConsolidationCustomer'].mean().reset_index()\n",
    "perc_consolidation_cust_eu_apa.columns = ['ItemNumber', 'ConsolidationCustomer']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_eu_apa = pd.merge(aggregated_df_new_eu_apa, perc_consolidation_cust_eu_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_consolidation_cust_eu = df_wide_data_items_in_order_eu.groupby('ItemNumber')['ConsolidationCustomer'].mean().reset_index()\n",
    "perc_consolidation_cust_eu.columns = ['ItemNumber', 'ConsolidationCustomer']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_eu = pd.merge(aggregated_df_new_eu, perc_consolidation_cust_eu, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_consolidation_cust_apa = df_wide_data_items_in_order_apa.groupby('ItemNumber')['ConsolidationCustomer'].mean().reset_index()\n",
    "perc_consolidation_cust_apa.columns = ['ItemNumber', 'ConsolidationCustomer']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_apa = pd.merge(aggregated_df_new_apa, perc_consolidation_cust_apa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_consolidation_cust_usa = df_wide_data_items_in_order_usa.groupby('ItemNumber')['ConsolidationCustomer'].mean().reset_index()\n",
    "perc_consolidation_cust_usa.columns = ['ItemNumber', 'ConsolidationCustomer']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new_usa = pd.merge(aggregated_df_new_usa, perc_consolidation_cust_usa, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_consolidation_cust = df_wide_data_items_in_order.groupby('ItemNumber')['ConsolidationCustomer'].mean().reset_index()\n",
    "perc_consolidation_cust.columns = ['ItemNumber', 'ConsolidationCustomer']\n",
    "\n",
    "# Merge the mean order quantity into emm_data_df\n",
    "aggregated_df_new = pd.merge(aggregated_df_new, perc_consolidation_cust, on='ItemNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_usa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_eu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_apa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add correlations and polynomial fit for numerical target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_apa['ResampledCurrentGLP'] = aggregated_df_new_apa['CurrentGLP'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), 4))\n",
    "aggregated_df_new_eu['ResampledCurrentGLP'] = aggregated_df_new_eu['CurrentGLP'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), 4))\n",
    "aggregated_df_new_eu_apa['ResampledCurrentGLP'] = aggregated_df_new_eu_apa['CurrentGLP'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), 4))\n",
    "aggregated_df_new_usa['ResampledCurrentGLP'] = aggregated_df_new_usa['CurrentGLP'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_apa['ResampledCurrentDailyQty'] = aggregated_df_new_apa['CurrentDailyQty'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), 4))\n",
    "aggregated_df_new_eu['ResampledCurrentDailyQty'] = aggregated_df_new_eu['CurrentDailyQty'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), 4))\n",
    "aggregated_df_new_eu_apa['ResampledCurrentDailyQty'] = aggregated_df_new_eu_apa['CurrentDailyQty'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), 4))\n",
    "aggregated_df_new_usa['ResampledCurrentDailyQty'] = aggregated_df_new_usa['CurrentDailyQty'].apply(\n",
    "    lambda arr: resample_array(np.array(arr), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(glp, qty):\n",
    "    try:\n",
    "        glp = np.array(glp)\n",
    "        qty = np.array(qty)\n",
    "        if len(glp) < 2 or len(qty) < 2:\n",
    "            return np.nan\n",
    "        if np.std(glp) == 0 or np.std(qty) == 0:\n",
    "            return np.nan\n",
    "        return np.corrcoef(glp, qty)[0, 1]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def compute_poly_fit(glp, qty):\n",
    "    try:\n",
    "        if len(glp) < 2 or len(qty) < 2:\n",
    "            return [np.nan, np.nan]\n",
    "        coeffs = np.polyfit(glp, qty, deg=1)  # [slope, intercept]\n",
    "        return coeffs.tolist()\n",
    "    except:\n",
    "        return [np.nan, np.nan]\n",
    "def compute_poly_fit_deg(glp, qty, deg):\n",
    "    try:\n",
    "        if len(glp) <= deg or len(qty) <= deg:\n",
    "            return [np.nan] * (deg + 1)\n",
    "        coeffs = np.polyfit(glp, qty, deg=deg)\n",
    "        return coeffs.tolist()\n",
    "    except:\n",
    "        return [np.nan] * (deg + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply functions to each row\n",
    "aggregated_df_new_apa[\"CorrGLPQty\"] = aggregated_df_new_apa.apply(\n",
    "    lambda row: compute_correlation(np.array(row[\"ResampledCurrentGLP\"]), np.array(row[\"ResampledCurrentDailyQty\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "aggregated_df_new_apa[\"PolyGLPQty\"] = aggregated_df_new_apa.apply(\n",
    "    lambda row: compute_poly_fit(np.array(row[\"ResampledCurrentGLP\"]), np.array(row[\"ResampledCurrentDailyQty\"])),\n",
    "    axis=1\n",
    ")\n",
    "# Apply degree 2 polynomial fit\n",
    "aggregated_df_new_apa[\"PolyDeg2GLPQty\"] = aggregated_df_new_apa.apply(\n",
    "    lambda row: compute_poly_fit_deg(\n",
    "        np.array(row[\"ResampledCurrentGLP\"]), \n",
    "        np.array(row[\"ResampledCurrentDailyQty\"]), \n",
    "        deg=2\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Apply degree 3 polynomial fit\n",
    "aggregated_df_new_apa[\"PolyDeg3GLPQty\"] = aggregated_df_new_apa.apply(\n",
    "    lambda row: compute_poly_fit_deg(\n",
    "        np.array(row[\"ResampledCurrentGLP\"]), \n",
    "        np.array(row[\"ResampledCurrentDailyQty\"]), \n",
    "        deg=3\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply functions to each row\n",
    "aggregated_df_new_eu[\"CorrGLPQty\"] = aggregated_df_new_eu.apply(\n",
    "    lambda row: compute_correlation(np.array(row[\"ResampledCurrentGLP\"]), np.array(row[\"ResampledCurrentDailyQty\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "aggregated_df_new_eu[\"PolyGLPQty\"] = aggregated_df_new_eu.apply(\n",
    "    lambda row: compute_poly_fit(np.array(row[\"ResampledCurrentGLP\"]), np.array(row[\"ResampledCurrentDailyQty\"])),\n",
    "    axis=1\n",
    ")\n",
    "# Apply degree 2 polynomial fit\n",
    "aggregated_df_new_eu[\"PolyDeg2GLPQty\"] = aggregated_df_new_eu.apply(\n",
    "    lambda row: compute_poly_fit_deg(\n",
    "        np.array(row[\"ResampledCurrentGLP\"]), \n",
    "        np.array(row[\"ResampledCurrentDailyQty\"]), \n",
    "        deg=2\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Apply degree 3 polynomial fit\n",
    "aggregated_df_new_eu[\"PolyDeg3GLPQty\"] = aggregated_df_new_eu.apply(\n",
    "    lambda row: compute_poly_fit_deg(\n",
    "        np.array(row[\"ResampledCurrentGLP\"]), \n",
    "        np.array(row[\"ResampledCurrentDailyQty\"]), \n",
    "        deg=3\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply functions to each row\n",
    "aggregated_df_new_eu_apa[\"CorrGLPQty\"] = aggregated_df_new_eu_apa.apply(\n",
    "    lambda row: compute_correlation(np.array(row[\"ResampledCurrentGLP\"]), np.array(row[\"ResampledCurrentDailyQty\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "aggregated_df_new_eu_apa[\"PolyGLPQty\"] = aggregated_df_new_eu_apa.apply(\n",
    "    lambda row: compute_poly_fit(np.array(row[\"ResampledCurrentGLP\"]), np.array(row[\"ResampledCurrentDailyQty\"])),\n",
    "    axis=1\n",
    ")\n",
    "# Apply degree 2 polynomial fit\n",
    "aggregated_df_new_eu_apa[\"PolyDeg2GLPQty\"] = aggregated_df_new_eu_apa.apply(\n",
    "    lambda row: compute_poly_fit_deg(\n",
    "        np.array(row[\"ResampledCurrentGLP\"]), \n",
    "        np.array(row[\"ResampledCurrentDailyQty\"]), \n",
    "        deg=2\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Apply degree 3 polynomial fit\n",
    "aggregated_df_new_eu_apa[\"PolyDeg3GLPQty\"] = aggregated_df_new_eu_apa.apply(\n",
    "    lambda row: compute_poly_fit_deg(\n",
    "        np.array(row[\"ResampledCurrentGLP\"]), \n",
    "        np.array(row[\"ResampledCurrentDailyQty\"]), \n",
    "        deg=3\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply functions to each row\n",
    "aggregated_df_new_usa[\"CorrGLPQty\"] = aggregated_df_new_usa.apply(\n",
    "    lambda row: compute_correlation(np.array(row[\"ResampledCurrentGLP\"]), np.array(row[\"ResampledCurrentDailyQty\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "aggregated_df_new_usa[\"PolyGLPQty\"] = aggregated_df_new_usa.apply(\n",
    "    lambda row: compute_poly_fit(np.array(row[\"ResampledCurrentGLP\"]), np.array(row[\"ResampledCurrentDailyQty\"])),\n",
    "    axis=1\n",
    ")\n",
    "# Apply degree 2 polynomial fit\n",
    "aggregated_df_new_usa[\"PolyDeg2GLPQty\"] = aggregated_df_new_usa.apply(\n",
    "    lambda row: compute_poly_fit_deg(\n",
    "        np.array(row[\"ResampledCurrentGLP\"]), \n",
    "        np.array(row[\"ResampledCurrentDailyQty\"]), \n",
    "        deg=2\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Apply degree 3 polynomial fit\n",
    "aggregated_df_new_usa[\"PolyDeg3GLPQty\"] = aggregated_df_new_usa.apply(\n",
    "    lambda row: compute_poly_fit_deg(\n",
    "        np.array(row[\"ResampledCurrentGLP\"]), \n",
    "        np.array(row[\"ResampledCurrentDailyQty\"]), \n",
    "        deg=3\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_usa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aggregated_df_new.to_csv('AggregatedDataALL.csv', index=False)\n",
    "\n",
    "aggregated_df_new_usa.to_csv('AggregatedDataUSA.csv', index=False)\n",
    "aggregated_df_new_eu_apa.to_csv('AggregatedDataEUandAPA.csv', index=False)\n",
    "aggregated_df_new.to_pickle('AggregatedDataALL.pkl')\n",
    "aggregated_df_new_usa.to_pickle('AggregatedDataUSA.pkl')\n",
    "aggregated_df_new_eu_apa.to_pickle('AggregatedDataEUandAPA.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_eu.to_csv('AggregatedDataEU.csv', index=False)\n",
    "aggregated_df_new_eu.to_pickle('AggregatedDataEU.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_new_apa.to_csv('AggregatedDataAPA.csv', index=False)\n",
    "aggregated_df_new_apa.to_pickle('AggregatedDataAPA.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
