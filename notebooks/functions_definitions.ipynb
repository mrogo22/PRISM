{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysubgroup as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import os\n",
    "import csv\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_array(arr, fixed_length):\n",
    "    \"\"\"\n",
    "    Resample a 1D numpy array to a fixed length using linear interpolation.\n",
    "\n",
    "    Parameters:\n",
    "        arr (array-like): Original array of numeric values.\n",
    "        fixed_length (int): Desired length of the resampled array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resampled array of length 'fixed_length'.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    original_length = len(arr)\n",
    "\n",
    "    if original_length == fixed_length:\n",
    "        return arr\n",
    "\n",
    "    # Create original and new equally spaced indices (scaled from 0 to 1)\n",
    "    original_indices = np.linspace(0, 1, original_length)\n",
    "    target_indices = np.linspace(0, 1, fixed_length)\n",
    "\n",
    "    # Use linear interpolation to compute resampled values\n",
    "    resampled = np.interp(target_indices, original_indices, arr)\n",
    "\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sd_array_magnitude(df, initial_data, target_col, fixed_length=None, min_size_sg = None, alpha = None, result_set_size=None, depth=None):\n",
    "    if result_set_size is None:\n",
    "        result_set_size = 1\n",
    "    if depth is None:\n",
    "        depth = 10\n",
    "    if fixed_length is None:\n",
    "        fixed_length = 8\n",
    "    if min_size_sg is None:\n",
    "        min_size_sg = 100\n",
    "    if alpha is None:\n",
    "        alpha = 0.5\n",
    "\n",
    "    data = df\n",
    "    target = ps.ArrayTarget(target_col, fixed_length = fixed_length, initial_data= initial_data)\n",
    "    searchspace = ps.create_selectors(data, ignore=target_col)\n",
    "    task = ps.SubgroupDiscoveryTask (\n",
    "        data,\n",
    "        target,\n",
    "        searchspace,\n",
    "        result_set_size=result_set_size,\n",
    "        depth=depth,\n",
    "        qf=ps.ArraySignMagnitudeQF(alpha = alpha, fixed_length = fixed_length, min_size_sg = min_size_sg, initial_data= initial_data))\n",
    "    result = ps.DFS().execute(task)\n",
    "\n",
    "    return result.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_array_from_string(s):\n",
    "    \"\"\"\n",
    "    Parse a string representing an array of numbers separated by whitespace or commas.\n",
    "    \n",
    "    Examples:\n",
    "        s = \"[-1.33 -1.63 -1.94 -2.37]\"\n",
    "        s = \"[-0.82, 0.58, 2.72, 1.89]\"\n",
    "    \n",
    "    Returns:\n",
    "        np.array: A NumPy array of floats.\n",
    "    \"\"\"\n",
    "    # Remove surrounding whitespace and brackets\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        s = s[1:-1].strip()\n",
    "    elif s.startswith('\"[') and s.endswith(']\"'):\n",
    "        s = s[2:-2].strip()\n",
    "\n",
    "    # Replace commas with spaces so we can split regardless of separator\n",
    "    s = s.replace(\",\", \" \")\n",
    "\n",
    "    try:\n",
    "        numbers = [float(item) for item in s.split()]\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Error parsing string into floats: {s}\") from e\n",
    "\n",
    "    return np.array(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_array(init_results):\n",
    "    quality = init_results.iloc[0, 0]\n",
    "    sg_des = init_results.iloc[0, 1]\n",
    "    size_sg = init_results.iloc[0, 2]\n",
    "    size_cover_all = init_results.iloc[0, 3]\n",
    "    covered_not_in_sg = init_results.iloc[0, 4]\n",
    "    size_dataset = init_results.iloc[0, 5]\n",
    "    centroid_sg = init_results.iloc[0, 6]\n",
    "    centroid_dataset = init_results.iloc[0, 7]\n",
    "    avg_cosine = init_results.iloc[0, 8]\n",
    "    sign_consistency = init_results.iloc[0, 9]\n",
    "\n",
    "    return quality, sg_des, size_sg,size_cover_all,covered_not_in_sg, size_dataset, centroid_sg, centroid_dataset, avg_cosine, sign_consistency\n",
    "\n",
    "def append_and_cut_array(best_results, init_results, df, sg_des, size_sg, discovered_so_far):\n",
    "    best_results.append(init_results.iloc[0])\n",
    "    mask = sg_des.covers(df)\n",
    "    remaining_data = df[~mask]\n",
    "    discovered_so_far += size_sg\n",
    "\n",
    "    return best_results, remaining_data, discovered_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peeling_magnitude(df, target_col, alpha_val, min_size_sg_val):\n",
    "    \n",
    "    print(\"Starting new run.\")\n",
    "    best_results = []\n",
    "    discovered_so_far = 0\n",
    "    iteration = 1\n",
    "    new_min = min_size_sg_val\n",
    "    remaining_data = df\n",
    "    # Initial subgroup discovery using the magnitude-based quality function.\n",
    "    init_results = run_sd_array_magnitude(df,df, target_col, alpha=alpha_val, min_size_sg=min_size_sg_val)\n",
    "    quality, sg_des, size_sg,size_cover_all,covered_not_in_sg, size_dataset, centroid_sg, centroid_dataset, avg_cosine, sign_consistency = get_attr_array(init_results)\n",
    "    \n",
    "    if quality < 0.5:\n",
    "        print('No satisfactory subgroup found, quality = ', quality, \" and sg size = \", size_sg, \" and remaining data = \", len(remaining_data))\n",
    "    else:\n",
    "        print('Initial satisfactory subgroup found, quality = ', quality, \" and sg size = \", size_sg, \" and remaining data = \", len(remaining_data))\n",
    "        best_results, remaining_data, discovered_so_far = append_and_cut_array(\n",
    "            best_results, init_results, df, sg_des, size_sg, discovered_so_far\n",
    "        )\n",
    "    \n",
    "    print(\"Starting recursion\")\n",
    "    while quality > 0.5 and len(remaining_data) > 100:\n",
    "        if quality < 0.6:\n",
    "            new_min = max(5, new_min - 10)\n",
    "        iteration += 1\n",
    "        print(\"Starting iteration: \", iteration)\n",
    "        result = run_sd_array_magnitude(remaining_data,df, target_col, alpha=alpha_val, min_size_sg=new_min)\n",
    "        quality, sg_des, size_sg,size_cover_all,covered_not_in_sg, size_dataset, centroid_sg, centroid_dataset, avg_cosine, sign_consistency = get_attr_array(result)\n",
    "        print(\"Finished iteration: \", iteration, \" with quality = \", quality, \" and sg size = \", size_sg, \" and remaining data = \", len(remaining_data))\n",
    "        new_min = min_size_sg_val\n",
    "        if quality < 0.5 or len(remaining_data) < 100:\n",
    "            print(\"Quality fell below 0.5 or remaining data is less than 100, stopping search\")\n",
    "            break\n",
    "        best_results, remaining_data, discovered_so_far = append_and_cut_array(\n",
    "            best_results, result, remaining_data, sg_des, size_sg, discovered_so_far\n",
    "        )\n",
    "        \n",
    "    \n",
    "    print(\"Ended recursive search, results are final\")\n",
    "    print(\"Items discovered so far:\", discovered_so_far)\n",
    "    print(\"Items left to classify:\", len(remaining_data))\n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_peeling_EMM(df, target_col, alpha_val, min_size_sg_val):\n",
    "    \n",
    "    print(\"Starting new run.\")\n",
    "    best_results = []\n",
    "    discovered_so_far = 0\n",
    "    iteration = 1\n",
    "    new_min = min_size_sg_val\n",
    "    remaining_data = df\n",
    "    # Initial subgroup discovery using the magnitude-based quality function.\n",
    "    init_results = run_EMM(df,df, target_col, alpha=alpha_val, min_size_sg=min_size_sg_val)\n",
    "    quality, sg_des, size_sg = get_attr_array(init_results)\n",
    "    \n",
    "    if quality < 0.5:\n",
    "        print('No satisfactory subgroup found, quality = ', quality, \" and sg size = \", size_sg, \" and remaining data = \", len(remaining_data))\n",
    "    else:\n",
    "        print('Initial satisfactory subgroup found, quality = ', quality, \" and sg size = \", size_sg, \" and remaining data = \", len(remaining_data))\n",
    "        best_results, remaining_data, discovered_so_far = append_and_cut_array(\n",
    "            best_results, init_results, df, sg_des, size_sg, discovered_so_far\n",
    "        )\n",
    "    \n",
    "    print(\"Starting recursion\")\n",
    "    while quality > 0.5 and len(remaining_data) > 100:\n",
    "        if quality < 0.6:\n",
    "            new_min = max(5, new_min - 10)\n",
    "        iteration += 1\n",
    "        print(\"Starting iteration: \", iteration)\n",
    "        result = run_EMM(remaining_data,df, target_col, alpha=alpha_val, min_size_sg=new_min)\n",
    "        quality, sg_des, size_sg = get_attr_array(result)\n",
    "        print(\"Finished iteration: \", iteration, \" with quality = \", quality, \" and sg size = \", size_sg, \" and remaining data = \", len(remaining_data))\n",
    "        new_min = min_size_sg_val\n",
    "        if quality < 0.5 or len(remaining_data) < 100:\n",
    "            print(\"Quality fell below 0.5 or remaining data is less than 100, stopping search\")\n",
    "            break\n",
    "        best_results, remaining_data, discovered_so_far = append_and_cut_array(\n",
    "            best_results, result, remaining_data, sg_des, size_sg, discovered_so_far\n",
    "        )\n",
    "        \n",
    "    \n",
    "    print(\"Ended recursive search, results are final\")\n",
    "    print(\"Items discovered so far:\", discovered_so_far)\n",
    "    print(\"Items left to classify:\", len(remaining_data))\n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sg_dfs(wide_df, results_df):\n",
    "    # Dictionary to hold one DataFrame per subgroup\n",
    "    subgroup_dfs = {}\n",
    "    chopped_data = wide_df\n",
    "    # Loop through each row in cosine_array_recursive_results_df.\n",
    "    # It is assumed that each row has a subgroup description in the \"subgroup\" column.\n",
    "    for idx, row in results_df.iterrows():\n",
    "        subgroup_condition = row['subgroup']\n",
    "        try:\n",
    "            # Apply the filter to the original data to extract only the rows satisfying the subgroup condition.\n",
    "            mask = subgroup_condition.covers(chopped_data)\n",
    "            filtered_df = chopped_data[mask]\n",
    "            subgroup_dfs[subgroup_condition] = filtered_df\n",
    "            chopped_data = chopped_data[~chopped_data['ItemNumber'].isin(filtered_df['ItemNumber'])]\n",
    "            print(f\"Subgroup '{subgroup_condition}' has {len(filtered_df)} items.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error for subgroup '{subgroup_condition}': {e}\")\n",
    "    return subgroup_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sg_dfs_indexed(wide_df, results_df):\n",
    "    # Dictionary to hold one DataFrame per subgroup\n",
    "    subgroup_dfs = {}\n",
    "    chopped_data = wide_df\n",
    "    # Loop through each row in cosine_array_recursive_results_df.\n",
    "    # It is assumed that each row has a subgroup description in the \"subgroup\" column.\n",
    "    for idx, row in results_df.iterrows():\n",
    "        subgroup_condition = row['subgroup']\n",
    "        sg_index = row['Index']\n",
    "        try:\n",
    "            # Apply the filter to the original data to extract only the rows satisfying the subgroup condition.\n",
    "            mask = subgroup_condition.covers(chopped_data)\n",
    "            filtered_df = chopped_data[mask]\n",
    "            subgroup_dfs[sg_index] = filtered_df\n",
    "            chopped_data = chopped_data[~chopped_data['ItemNumber'].isin(filtered_df['ItemNumber'])]\n",
    "            print(f\"Subgroup '{subgroup_condition}' has {len(filtered_df)} items.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error for subgroup '{subgroup_condition}': {e}\")\n",
    "    return subgroup_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_turnover_per_year(order_data_eu_sg):\n",
    "    \"\"\"\n",
    "    Computes average turnover per year for a given order data subgroup,\n",
    "    excluding rows with abnormally high prices.\n",
    "\n",
    "    Parameters:\n",
    "        order_data_eu_sg (pd.DataFrame): Filtered order data for a subgroup.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of average yearly turnover values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify high price rows\n",
    "    high_price_rows = order_data_eu_sg[order_data_eu_sg['PriceSalesUoMEUR'] > 100000]\n",
    "    if not high_price_rows.empty:\n",
    "        print(\"⚠️ Warning: Data error in the group – rows with PriceSalesUoMEUR > 100000 were excluded.\")\n",
    "\n",
    "    # Exclude high price rows\n",
    "    clean_data = order_data_eu_sg[order_data_eu_sg['PriceSalesUoMEUR'] <= 100000]\n",
    "\n",
    "    # Compute total turnover per product per year\n",
    "    turnover_sum_per_product_year = (\n",
    "        clean_data.groupby(['ItemNumber', 'OrderYear'])['Turnover']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Compute total turnover per year per semester\n",
    "    turnover_sum_per_product_year_semester = (\n",
    "        clean_data.groupby(['ItemNumber', 'OrderYear', 'Semester'])['Turnover']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Compute average turnover per year\n",
    "    average_turnover_per_year = (\n",
    "        turnover_sum_per_product_year.groupby('OrderYear')['Turnover']\n",
    "        .mean()\n",
    "        .round()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Compute average turnover per year per semester\n",
    "    average_turnover_per_year_semester = (\n",
    "        turnover_sum_per_product_year_semester.groupby(['OrderYear', 'Semester'])['Turnover']\n",
    "        .mean()\n",
    "        .round()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return average_turnover_per_year_semester['Turnover'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_timespans(order_data_eu_sg):\n",
    "    # Identify high price rows\n",
    "    high_price_rows = order_data_eu_sg[order_data_eu_sg['PriceSalesUoMEUR'] > 100000]\n",
    "    if not high_price_rows.empty:\n",
    "        print(\"⚠️ Warning: Data error in the group – rows with PriceSalesUoMEUR > 100000 were excluded.\")\n",
    "\n",
    "    # Exclude high price rows\n",
    "    clean_data = order_data_eu_sg[order_data_eu_sg['PriceSalesUoMEUR'] <= 100000]\n",
    "\n",
    "    #exclude other years\n",
    "    clean_data = clean_data[clean_data['OrderYear'] >= 2021]\n",
    "    clean_data = clean_data[clean_data['OrderYear'] <= 2024]\n",
    "\n",
    "    # Compute the time span for each item\n",
    "     \n",
    "        # Group by ItemNumber and calculate the earliest and latest order years\n",
    "    order_year_range = clean_data.groupby('ItemNumber')['OrderYear'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "    # Rename the columns for clarity\n",
    "    order_year_range.columns = ['ItemNumber', 'EarliestOrderYear', 'LatestOrderYear']\n",
    "    order_year_range['Timespan'] = order_year_range['LatestOrderYear'] - order_year_range['EarliestOrderYear'] +1\n",
    "    mean_timespan = order_year_range['Timespan'].mean()\n",
    "\n",
    "    timespan_counts = order_year_range['Timespan'].value_counts()\n",
    "\n",
    "    \n",
    "    items_introduced_2021_count = timespan_counts.get(4, 0)\n",
    "    items_introduced_2022_count = timespan_counts.get(3, 0)\n",
    "    items_introduced_2023_count = timespan_counts.get(2, 0)\n",
    "    items_introduced_2024_count = timespan_counts.get(1, 0)\n",
    "\n",
    "\n",
    "    return mean_timespan, items_introduced_2021_count, items_introduced_2022_count, items_introduced_2023_count, items_introduced_2024_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_mint_forecast(\n",
    "    df_wide: pd.DataFrame,\n",
    "    h: int = 1,\n",
    "    seasonal: bool = False,\n",
    "    m: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform Hierarchical + (OLS‐style MinT) reconciliation on a subgroup.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_wide : DataFrame\n",
    "        Historical elasticity panel of shape (T × N), where\n",
    "        - index = TimePeriod (1…8 or actual dates)\n",
    "        - columns = ItemNumber\n",
    "        - values = IndividualPE\n",
    "    h : int\n",
    "        Forecast horizon (e.g. 1 for one‐step‐ahead).\n",
    "    seasonal : bool\n",
    "        Whether to include seasonal terms in auto_arima.\n",
    "    m : int\n",
    "        Seasonal period (only if seasonal=True).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    reconciled_df : DataFrame\n",
    "        Reconciled bottom‐level forecasts of shape (h × N), with the same\n",
    "        columns as df_wide, and a future index (TimePeriod T+1 … T+h).\n",
    "    \"\"\"\n",
    "    # 1) Build the top‐level (aggregate) series\n",
    "    #    You can sum or average; we’ll use the mean here:\n",
    "    agg_series = df_wide.mean(axis=1)\n",
    "\n",
    "    # 2) Fit ARIMA to the aggregate\n",
    "    print(\"Starting ARIMA - training model on subgroup ...\")\n",
    "    model_agg = auto_arima(\n",
    "        agg_series,\n",
    "        seasonal=seasonal,\n",
    "        m=m,\n",
    "        error_action='ignore',\n",
    "        suppress_warnings=True\n",
    "    )\n",
    "    print(\"Model trained on subgroup - staring general prediction...\")\n",
    "    #    Produce h‐step‐ahead aggregate forecast\n",
    "    f_agg = model_agg.predict(n_periods=h)            # shape (h,)\n",
    "    \n",
    "    # 3) Fit bottom‐level models (one per product)\n",
    "    product_ids = df_wide.columns.tolist()\n",
    "    print(f\"General prediction done! - starting individual product training on {len(product_ids)} products\")\n",
    "    bottom_forecasts = []\n",
    "    count = 0\n",
    "    passed_75 = False\n",
    "    passed_50 = False\n",
    "    passed_25 = False\n",
    "    for pid in product_ids:\n",
    "        count+=1\n",
    "        if count / len(product_ids) >= 0.75 and passed_75 == False:\n",
    "            print(\"75% of products trained!\")\n",
    "            passed_75 = True\n",
    "        if count / len(product_ids) >= 0.5 and passed_50 == False:\n",
    "            print(\"50% of products trained!\")\n",
    "            passed_50 = True\n",
    "        if count / len(product_ids) >= 0.25 and passed_25 == False:\n",
    "            print(\"25% of products trained!\")\n",
    "            passed_25 = True\n",
    "\n",
    "        m_b = auto_arima(\n",
    "            df_wide[pid],\n",
    "            seasonal=seasonal,\n",
    "            m=m,\n",
    "            error_action='ignore',\n",
    "            suppress_warnings=True\n",
    "        )\n",
    "        bottom_forecasts.append(m_b.predict(n_periods=h))\n",
    "    \n",
    "    print(\"Individual product training done! - starting reconciliation...\")\n",
    "\n",
    "    # Stack into array of shape (N_products × h)\n",
    "    f_b = np.vstack(bottom_forecasts)                 # shape (N, h)\n",
    "    n_products = f_b.shape[0]\n",
    "\n",
    "    # 4) OLS‐style MinT reconciliation (identity W)\n",
    "    #    Compute per‐horizon correction so bottoms sum to aggregate\n",
    "    correction = (f_agg - f_b.sum(axis=0)) / n_products  # shape (h,)\n",
    "    # force it into a 1-D NumPy array\n",
    "    correction = np.asarray(correction)\n",
    "    f_b_rec = f_b + correction[np.newaxis, :]           # shape (N, h)\n",
    "\n",
    "    # 5) Wrap into a DataFrame with a future TimePeriod index\n",
    "    last_period = df_wide.index.max()\n",
    "    future_index = range(last_period + 1, last_period + 1 + h)\n",
    "    reconciled_df = pd.DataFrame(\n",
    "        f_b_rec.T,               # transpose to (h, N)\n",
    "        index=future_index,\n",
    "        columns=product_ids\n",
    "    )\n",
    "    print(\"Reconciliation done! Results are ready.\")\n",
    "\n",
    "    return reconciled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define Directional Magnitude Score (DMS)\n",
    "def directional_magnitude_score(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    train: np.ndarray,\n",
    "    w: float = 0.5,\n",
    "    R: float = 25.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Directional Magnitude Score (0–1):\n",
    "      D = 1 if sign(y_pred) == sign(y_true), else 0\n",
    "      M = max(0, 1 - abs(y_pred - y_true) / R)\n",
    "      DMS = w * D + (1 - w) * M\n",
    "\n",
    "    Returns np.nan if train is constant.\n",
    "    \"\"\"\n",
    "    # Exclude constant-series\n",
    "    if np.allclose(train, train[0]):\n",
    "        return np.nan\n",
    "\n",
    "    # Flatten in case of multi-step h > 1\n",
    "    y_true = np.ravel(y_true)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "\n",
    "    scores = []\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        # 1) Direction correctness\n",
    "        D = 1.0 if np.sign(yt) == np.sign(yp) else 0.0\n",
    "        # 2) Magnitude closeness\n",
    "        M = max(0.0, 1.0 - abs(yp - yt) / R)\n",
    "        # 3) Combined score\n",
    "        scores.append(w * D + (1 - w) * M)\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "def evaluate_hierarchical_dms_all(\n",
    "    subgroup_indices,\n",
    "    wide_name_tpl,\n",
    "    folds: list = [5, 6, 7],\n",
    "    h: int = 1,\n",
    "    w: float = 0.5,\n",
    "    R: float = 25.0\n",
    "):\n",
    "    \"\"\"\n",
    "    For each subgroup:\n",
    "      • Computes the mean DMS (Directional Magnitude Score) across all products & folds,\n",
    "        prints it.\n",
    "      • Stores the mean DMS per product for later analysis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    subgroup_scores : dict\n",
    "        { idx -> mean DMS over products }\n",
    "    per_product_scores : dict\n",
    "        { idx -> { product_id -> mean DMS for that product } }\n",
    "    \"\"\"\n",
    "    subgroup_scores = {}\n",
    "    per_product_scores = {}\n",
    "\n",
    "    for idx in subgroup_indices:\n",
    "        # grab the wide panel from globals\n",
    "        df_wide = globals()[wide_name_tpl.format(idx=idx)]\n",
    "        \n",
    "        # prepare a place to collect per-fold DMS per product\n",
    "        errors = {pid: [] for pid in df_wide.columns}\n",
    "\n",
    "        # rolling‐origin backtest\n",
    "        for t_end in folds:\n",
    "            train    = df_wide.iloc[:t_end]\n",
    "            test     = df_wide.iloc[t_end : t_end + h]\n",
    "            forecast = hierarchical_mint_forecast(train, h=h)\n",
    "\n",
    "            for pid in df_wide.columns:\n",
    "                y_true       = test[pid].values\n",
    "                y_pred       = forecast[pid].values\n",
    "                train_series = train[pid].values\n",
    "                score = directional_magnitude_score(\n",
    "                    y_true, y_pred, train_series, w=w, R=R\n",
    "                )\n",
    "                errors[pid].append(score)\n",
    "\n",
    "        # average over folds for each product\n",
    "        mean_per_product = {\n",
    "            pid: float(np.nanmean(scores))\n",
    "            for pid, scores in errors.items()\n",
    "        }\n",
    "        per_product_scores[idx] = mean_per_product\n",
    "\n",
    "        # average those product‐means into one subgroup score\n",
    "        subgroup_mean = float(np.nanmean(list(mean_per_product.values())))\n",
    "        subgroup_scores[idx] = subgroup_mean\n",
    "\n",
    "        print(f\"Subgroup {idx}: mean DMS = {subgroup_mean:.3f}\")\n",
    "\n",
    "    return subgroup_scores, per_product_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_increase(glp_array):\n",
    "    if isinstance(glp_array, (list, np.ndarray)) and len(glp_array) > 1:\n",
    "        return max((glp_array[i + 1] - glp_array[i]) / glp_array[i] for i in range(len(glp_array) - 1))\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_for_row(row, i0, i1, k):\n",
    "    E     = row['Predicted PE']\n",
    "    p0    = row['LastGLP']\n",
    "    q0    = row['LastQty']\n",
    "    f_max = row['MaxIncrease']\n",
    "    r = i1 / i0\n",
    "    switch_point = 1.0 / (1.0 - 2.0 * r**2)\n",
    "    bump = k * E\n",
    "\n",
    "    # 1) E < switch_point\n",
    "    if E < switch_point:\n",
    "        P1_candidate = ((E - 1) / (2*E)) * (i0 / i1) * p0\n",
    "        Q1_candidate = (1 - E) * q0 / 2\n",
    "\n",
    "        if P1_candidate * Q1_candidate > p0 * q0:\n",
    "            return P1_candidate\n",
    "        else:\n",
    "            # fallback to indexation\n",
    "            return p0 * (i1 / i0)\n",
    "\n",
    "    # 2) switch_point <= E < 0\n",
    "    if E >= switch_point and E < 0:\n",
    "        return (i1 / i0) * p0\n",
    "\n",
    "    # 3) E >= 0\n",
    "    if f_max >= 0:\n",
    "        bump = min(bump, f_max)\n",
    "    return p0 * r * (1 + bump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_qty_per_row(row, i0, i1):\n",
    "    E     = row['Predicted PE']\n",
    "    p0    = row['LastGLP']\n",
    "    p1   = row['Recommended GLP']\n",
    "    r = i1 / i0\n",
    "    switch_point = 1.0 / (1.0 - 2.0 * r**2)\n",
    "    q0   = row['LastQty']\n",
    "\n",
    "    # 1) E < switch_point\n",
    "    if E < switch_point and p1<p0:\n",
    "        q1 = (1-E)*q0/2\n",
    "    \n",
    "    elif E < switch_point and p1>=p0:\n",
    "        q1 =  q0 \n",
    "    # 2) switch_point <= E < 0\n",
    "    elif E >= switch_point and E < 0:\n",
    "        q1 =  q0\n",
    "\n",
    "    # 3) E >= 0\n",
    "    else:\n",
    "        q1 = q0 * ((p1 / p0) * (i1 / i0))**E\n",
    "\n",
    "    return min(q1, 3*q0)  # cap at 3 times the original quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_check_row(row):\n",
    "    q0 = row['LastQty']\n",
    "    p0 = row['LastGLP']\n",
    "    p1 = row['Recommended GLP']\n",
    "    q1 = row['EstQty']\n",
    "    return p1*q1 - p0*q0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
